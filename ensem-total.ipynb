{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "630e4d8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:26:00.621677Z",
     "iopub.status.busy": "2021-09-27T11:26:00.620184Z",
     "iopub.status.idle": "2021-09-27T11:26:01.529250Z",
     "shell.execute_reply": "2021-09-27T11:26:01.528689Z"
    },
    "papermill": {
     "duration": 0.948364,
     "end_time": "2021-09-27T11:26:01.529429",
     "exception": false,
     "start_time": "2021-09-27T11:26:00.581065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import numpy.matlib\n",
    "\n",
    "target_name = 'target'\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00f70c96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:26:01.605265Z",
     "iopub.status.busy": "2021-09-27T11:26:01.604229Z",
     "iopub.status.idle": "2021-09-27T11:26:03.136517Z",
     "shell.execute_reply": "2021-09-27T11:26:03.137556Z"
    },
    "papermill": {
     "duration": 1.570379,
     "end_time": "2021-09-27T11:26:03.137723",
     "exception": false,
     "start_time": "2021-09-27T11:26:01.567344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    1.3s finished\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../input/optiver-realized-volatility-prediction/'\n",
    "\n",
    "\n",
    "def calc_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "\n",
    "def log_return(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n",
    "\n",
    "\n",
    "def read_test():\n",
    "    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n",
    "    \n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    return test\n",
    "\n",
    "\n",
    "def book_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    \n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    \n",
    "    df['wap_mean'] = (df['wap1'] + df['wap2']) / 2\n",
    "    \n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
    "    \n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "    \n",
    "    df['bas'] = (df[['ask_price1', 'ask_price2']].min(axis = 1)/ df[['bid_price1', 'bid_price2']].max(axis = 1) - 1) \n",
    "    df['h_spread_l1'] = df['ask_price1'] - df['bid_price1']\n",
    "    df['h_spread_l2'] = df['ask_price2'] - df['bid_price2']\n",
    "    df['log_return_bid_price1'] = np.log(df['bid_price1'].pct_change() + 1)\n",
    "    df['log_return_ask_price1'] = np.log(df['ask_price1'].pct_change() + 1)\n",
    "    df['log_return_bid_size1'] = np.log(df['bid_size1'].pct_change() + 1)\n",
    "    df['log_return_ask_size1'] = np.log(df['ask_size1'].pct_change() + 1)\n",
    "    df['log_ask_1_div_bid_1'] = np.log(df['ask_price1'] / df['bid_price1'])\n",
    "    df['log_ask_1_div_bid_1_size'] = np.log(df['ask_size1'] / df['bid_size1'])\n",
    "    df['log_return_bid_price2'] = np.log(df['bid_price2'].pct_change() + 1)\n",
    "    df['log_return_ask_price2'] = np.log(df['ask_price2'].pct_change() + 1)\n",
    "    df['log_return_bid_size2'] = np.log(df['bid_size2'].pct_change() + 1)\n",
    "    df['log_return_ask_size2'] = np.log(df['ask_size2'].pct_change() + 1)\n",
    "    df['log_ask_2_div_bid_2'] = np.log(df['ask_price2'] / df['bid_price2'])\n",
    "    df['log_ask_2_div_bid_2_size'] = np.log(df['ask_size2'] / df['bid_size2'])\n",
    "    \n",
    "    \n",
    "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
    "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
    "    \n",
    "    \n",
    "    create_feature_dict = {\n",
    "        'wap1': [np.mean,np.max,np.std],\n",
    "        'wap2': [np.mean,np.max,np.std],\n",
    "\n",
    "        'log_return1': [realized_volatility,np.sum],\n",
    "        'log_return2': [realized_volatility,np.sum], \n",
    "        'log_return_bid_price1':[realized_volatility,np.sum], \n",
    "        'log_return_ask_price1':[realized_volatility,np.sum],\n",
    "        'log_return_bid_size1':[realized_volatility,np.sum], \n",
    "        'log_return_ask_size1':[realized_volatility,np.sum], \n",
    "        'log_ask_1_div_bid_1':[realized_volatility,np.sum], \n",
    "        'log_ask_1_div_bid_1_size':[realized_volatility,np.sum], \n",
    "        'log_return_bid_price2':[realized_volatility,np.sum], \n",
    "        'log_return_ask_price2':[realized_volatility,np.sum], \n",
    "        'log_return_bid_size2':[realized_volatility,np.sum], \n",
    "        'log_return_ask_size2':[realized_volatility,np.sum], \n",
    "        'log_ask_2_div_bid_2':[realized_volatility,np.sum], \n",
    "        'log_ask_2_div_bid_2_size':[realized_volatility,np.sum], \n",
    " \n",
    "        'wap_balance': [np.mean,np.max,np.std,np.sum], \n",
    "        'wap_mean': [np.mean,np.max,np.std,np.sum],\n",
    "        'price_spread':[np.mean,np.max,np.std,np.sum],\n",
    "        'bid_spread':[np.mean,np.max,np.std,np.sum],\n",
    "        'ask_spread':[np.mean,np.max,np.std,np.sum],\n",
    "        'total_volume':[np.mean,np.max,np.std,np.sum],\n",
    "        'volume_imbalance':[np.mean,np.max,np.std,np.sum],\n",
    "        'bas':[np.mean,np.max,np.std,np.sum],\n",
    "        'h_spread_l1':[np.mean,np.max,np.std,np.sum],\n",
    "        'h_spread_l2':[np.mean,np.max,np.std,np.sum],\n",
    "        'price_spread2':[np.mean,np.max,np.std,np.sum],\n",
    "        \"bid_ask_spread\":[np.mean,np.max,np.std,np.sum]}\n",
    "    \n",
    "    \n",
    "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
    "        \n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n",
    "        \n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        \n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    \n",
    "    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n",
    "\n",
    "    \n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "\n",
    "def trade_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    \n",
    "    \n",
    "    create_feature_dict = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n",
    "        'order_count':[np.mean,np.sum,np.max],\n",
    "    }\n",
    "    \n",
    "    \n",
    "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
    "        \n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n",
    "        \n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        \n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "\n",
    "    \n",
    "    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n",
    "    \n",
    "    def tendency(price, vol):    \n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff/price[1:])*100\n",
    "        power = np.sum(val*vol[1:])\n",
    "        return(power)\n",
    "    \n",
    "    lis = []\n",
    "    for n_time_id in df['time_id'].unique():\n",
    "        df_id = df[df['time_id'] == n_time_id]        \n",
    "        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n",
    "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
    "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
    "        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n",
    "        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n",
    "        \n",
    "        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n",
    "        energy = np.mean(df_id['price'].values**2)\n",
    "        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n",
    "        \n",
    "        \n",
    "        \n",
    "        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n",
    "        energy_v = np.sum(df_id['size'].values**2)\n",
    "        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n",
    "        \n",
    "        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n",
    "                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n",
    "    \n",
    "    df_lr = pd.DataFrame(lis)\n",
    "        \n",
    "   \n",
    "    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
    "    \n",
    "    \n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    \n",
    "    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200','time_id'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "\n",
    "def get_time_stock(df):\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n",
    "    \n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    \n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    \n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    \n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "    \n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
    "    return df\n",
    "    \n",
    "\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    \n",
    "   \n",
    "    def for_joblib(stock_id):\n",
    "        \n",
    "        if is_train:\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        \n",
    "        else:\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    "        \n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
    "        \n",
    "        \n",
    "        return df_tmp\n",
    "    \n",
    "    \n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "    \n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "test = read_test()\n",
    "\n",
    "\n",
    "test_stock_ids = test['stock_id'].unique()\n",
    "\n",
    "test_ = preprocessor(test_stock_ids, is_train = False)\n",
    "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
    "\n",
    "\n",
    "test = get_time_stock(test)\n",
    "\n",
    "test['size_tau'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique'] )\n",
    "test['size_tau_400'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_400'] )\n",
    "test['size_tau_300'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_300'] )\n",
    "test['size_tau_200'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_200'] )\n",
    "test['size_tau2'] = np.sqrt( 1/ test['trade_order_count_sum'] )\n",
    "test['size_tau2_400'] = np.sqrt( 0.25/ test['trade_order_count_sum'] )\n",
    "test['size_tau2_300'] = np.sqrt( 0.5/ test['trade_order_count_sum'] )\n",
    "test['size_tau2_200'] = np.sqrt( 0.75/ test['trade_order_count_sum'] )\n",
    "\n",
    "test['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a7c592a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:26:03.221660Z",
     "iopub.status.busy": "2021-09-27T11:26:03.220996Z",
     "iopub.status.idle": "2021-09-27T11:26:12.551864Z",
     "shell.execute_reply": "2021-09-27T11:26:12.551371Z"
    },
    "papermill": {
     "duration": 9.375555,
     "end_time": "2021-09-27T11:26:12.551993",
     "exception": false,
     "start_time": "2021-09-27T11:26:03.176438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-27 11:26:03.718224: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(21)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(21)\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.backend import sigmoid\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow import keras\n",
    "def root_mean_squared_per_error(y_true, y_pred):\n",
    "         return K.sqrt(K.mean(K.square( (y_true - y_pred)/ y_true )))\n",
    "    \n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=20, verbose=0,\n",
    "    mode='min',restore_best_weights=True)\n",
    "\n",
    "plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.2, patience=7, verbose=0,\n",
    "    mode='min')\n",
    "colNames = list(test)\n",
    "colNames.remove('time_id')\n",
    "colNames.remove('row_id')\n",
    "colNames.remove('stock_id')\n",
    "test.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "from pickle import load,dump\n",
    "for col in colNames:\n",
    "    qt=load(open(f'/kaggle/input/ng-wwdd/qt_{col}.pkl', 'rb'))\n",
    "    test[col] = qt.transform(test[[col]])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dc0bf57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:26:12.639062Z",
     "iopub.status.busy": "2021-09-27T11:26:12.638397Z",
     "iopub.status.idle": "2021-09-27T11:26:19.523190Z",
     "shell.execute_reply": "2021-09-27T11:26:19.523744Z"
    },
    "papermill": {
     "duration": 6.937006,
     "end_time": "2021-09-27T11:26:19.523938",
     "exception": false,
     "start_time": "2021-09-27T11:26:12.586932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 11, 22, 50, 55, 56, 62, 73, 76, 78, 84, 87, 96, 101, 112, 116, 122, 124, 126]\n",
      "[0, 4, 5, 10, 15, 16, 17, 23, 26, 28, 29, 36, 42, 44, 48, 53, 66, 69, 72, 85, 94, 95, 100, 102, 109, 111, 113, 115, 118, 120]\n",
      "[3, 6, 9, 18, 61, 63, 86, 97]\n",
      "[27, 31, 33, 37, 38, 40, 58, 59, 60, 74, 75, 77, 82, 83, 88, 89, 90, 98, 99, 110]\n",
      "[2, 7, 13, 14, 19, 20, 21, 30, 32, 34, 35, 39, 41, 43, 46, 47, 51, 52, 64, 67, 68, 70, 93, 103, 104, 105, 107, 108, 114, 119, 123, 125]\n",
      "[81]\n",
      "[8, 80]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:33: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "corr = train_p.corr()\n",
    "\n",
    "ids = corr.index\n",
    "\n",
    "label=[1, 0, 4, 2, 1, 1, 2, 4, 6, 2, 1, 0, 4, 4, 1, 1, 1, 2, 4, 4, 4, 0, 1, 1, 3, 1, 1, 4, 3, 4, 3, 4, 4, 1, 3, 3, 4,\n",
    "       3, 4, 1, 4, 1, 4, 4, 1, 0, 4, 4, 1, 0, 0, 3, 3, 3, 2, 0, 2, 4, 1, 4, 4, 1, 4, 1, 0, 3, 3, 0, 3, 0, 6, 5, 3, 3,\n",
    "       0, 1, 2, 0, 3, 3, 3, 4, 1, 1, 0, 2, 3, 3, 1, 0, 1, 4, 4, 4, 4, 4, 1, 3, 1, 0, 1, 4, 1, 0, 1, 4, 1, 0, 4, 0, 4,\n",
    "       0]\n",
    "\n",
    "l = []\n",
    "for n in range(7):\n",
    "    l.append ( [ (x-1) for x in ( (ids+1)*(label==n*np.ones(len(label)))) if x > 0] )\n",
    "    \n",
    "matTest = []\n",
    "\n",
    "n = 0\n",
    "for ind in l:\n",
    "    print(ind)\n",
    "    newDf = test.loc[test['stock_id'].isin(ind) ]    \n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    matTest.append ( newDf )\n",
    "    \n",
    "    n+=1  \n",
    "mat2 = pd.concat(matTest).reset_index()\n",
    "mat1=pd.read_csv('/kaggle/input/ng-wwdd/mat1.csv')\n",
    "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n",
    "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "mat2.reset_index(inplace=True)\n",
    "nnn = ['time_id',\n",
    "     'log_return1_realized_volatility_0c1',\n",
    "     'log_return1_realized_volatility_1c1',     \n",
    "     'log_return1_realized_volatility_3c1',\n",
    "     'log_return1_realized_volatility_4c1',     \n",
    "     'log_return1_realized_volatility_2c1',\n",
    "     'total_volume_mean_0c1',\n",
    "     'total_volume_mean_1c1', \n",
    "     'total_volume_mean_3c1',\n",
    "     'total_volume_mean_4c1', \n",
    "     'total_volume_mean_2c1',\n",
    "     'trade_size_mean_0c1',\n",
    "     'trade_size_mean_1c1', \n",
    "     'trade_size_mean_3c1',\n",
    "     'trade_size_mean_4c1', \n",
    "     'trade_size_mean_2c1',\n",
    "     'trade_order_count_mean_0c1',\n",
    "     'trade_order_count_mean_1c1',\n",
    "     'trade_order_count_mean_3c1',\n",
    "     'trade_order_count_mean_4c1',\n",
    "     'trade_order_count_mean_2c1',      \n",
    "     'price_spread_mean_0c1',\n",
    "     'price_spread_mean_1c1',\n",
    "     'price_spread_mean_3c1',\n",
    "     'price_spread_mean_4c1',\n",
    "     'price_spread_mean_2c1',   \n",
    "     'bid_spread_mean_0c1',\n",
    "     'bid_spread_mean_1c1',\n",
    "     'bid_spread_mean_3c1',\n",
    "     'bid_spread_mean_4c1',\n",
    "     'bid_spread_mean_2c1',       \n",
    "     'ask_spread_mean_0c1',\n",
    "     'ask_spread_mean_1c1',\n",
    "     'ask_spread_mean_3c1',\n",
    "     'ask_spread_mean_4c1',\n",
    "     'ask_spread_mean_2c1',   \n",
    "     'volume_imbalance_mean_0c1',\n",
    "     'volume_imbalance_mean_1c1',\n",
    "     'volume_imbalance_mean_3c1',\n",
    "     'volume_imbalance_mean_4c1',\n",
    "     'volume_imbalance_mean_2c1',       \n",
    "     'bid_ask_spread_mean_0c1',\n",
    "     'bid_ask_spread_mean_1c1',\n",
    "     'bid_ask_spread_mean_3c1',\n",
    "     'bid_ask_spread_mean_4c1',\n",
    "     'bid_ask_spread_mean_2c1',\n",
    "     'size_tau2_0c1',\n",
    "     'size_tau2_1c1',\n",
    "     'size_tau2_3c1',\n",
    "     'size_tau2_4c1',\n",
    "     'size_tau2_2c1'] \n",
    "test = pd.merge(test,mat2[nnn],how='left',on='time_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b734d5d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:26:19.605515Z",
     "iopub.status.busy": "2021-09-27T11:26:19.599148Z",
     "iopub.status.idle": "2021-09-27T11:26:19.882298Z",
     "shell.execute_reply": "2021-09-27T11:26:19.881836Z"
    },
    "papermill": {
     "duration": 0.321233,
     "end_time": "2021-09-27T11:26:19.882443",
     "exception": false,
     "start_time": "2021-09-27T11:26:19.561210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def swish(x, beta = 1):\n",
    "    return (x * sigmoid(beta * x))\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "hidden_units = (128,64,32)\n",
    "stock_embedding_size = 24\n",
    "\n",
    "train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "cat_data =train['stock_id']\n",
    "del train\n",
    "gc.collect()\n",
    "\n",
    "def base_model():\n",
    "    \n",
    "    \n",
    "    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n",
    "    num_input = keras.Input(shape=(538,), name='num_data')\n",
    "\n",
    "\n",
    "    \n",
    "    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n",
    "                                           input_length=1, name='stock_embedding')(stock_id_input)\n",
    "    stock_flattened = keras.layers.Flatten()(stock_embedded)\n",
    "    out = keras.layers.Concatenate()([stock_flattened, num_input])\n",
    "    \n",
    "    \n",
    "    for n_hidden in hidden_units:\n",
    "\n",
    "        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
    "    \n",
    "    model = keras.Model(\n",
    "    inputs = [stock_id_input, num_input],\n",
    "    outputs = out,\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f91f46f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:26:19.958130Z",
     "iopub.status.busy": "2021-09-27T11:26:19.957618Z",
     "iopub.status.idle": "2021-09-27T11:26:20.044144Z",
     "shell.execute_reply": "2021-09-27T11:26:20.043638Z"
    },
    "papermill": {
     "duration": 0.126258,
     "end_time": "2021-09-27T11:26:20.044263",
     "exception": false,
     "start_time": "2021-09-27T11:26:19.918005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_to_consider = list(test)\n",
    "features_to_consider.remove('time_id')\n",
    "features_to_consider.remove('row_id')\n",
    "f_mean = np.load('/kaggle/input/ng-wwdd/f_mean.npy')\n",
    "for i in range(len(features_to_consider)):\n",
    "    test[features_to_consider[i]] = test[features_to_consider[i]].fillna(f_mean[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de058cb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:26:20.123073Z",
     "iopub.status.busy": "2021-09-27T11:26:20.122308Z",
     "iopub.status.idle": "2021-09-27T11:26:20.146891Z",
     "shell.execute_reply": "2021-09-27T11:26:20.147301Z"
    },
    "papermill": {
     "duration": 0.067736,
     "end_time": "2021-09-27T11:26:20.147447",
     "exception": false,
     "start_time": "2021-09-27T11:26:20.079711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>row_id</th>\n",
       "      <th>wap1_mean</th>\n",
       "      <th>wap1_amax</th>\n",
       "      <th>wap1_std</th>\n",
       "      <th>wap2_mean</th>\n",
       "      <th>wap2_amax</th>\n",
       "      <th>wap2_std</th>\n",
       "      <th>log_return1_realized_volatility</th>\n",
       "      <th>...</th>\n",
       "      <th>bid_ask_spread_mean_0c1</th>\n",
       "      <th>bid_ask_spread_mean_1c1</th>\n",
       "      <th>bid_ask_spread_mean_3c1</th>\n",
       "      <th>bid_ask_spread_mean_4c1</th>\n",
       "      <th>bid_ask_spread_mean_2c1</th>\n",
       "      <th>size_tau2_0c1</th>\n",
       "      <th>size_tau2_1c1</th>\n",
       "      <th>size_tau2_3c1</th>\n",
       "      <th>size_tau2_4c1</th>\n",
       "      <th>size_tau2_2c1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0-4</td>\n",
       "      <td>0.250325</td>\n",
       "      <td>-0.543764</td>\n",
       "      <td>-2.429043</td>\n",
       "      <td>0.335747</td>\n",
       "      <td>-0.527097</td>\n",
       "      <td>-2.723074</td>\n",
       "      <td>-3.421779</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.279017</td>\n",
       "      <td>0.823808</td>\n",
       "      <td>0.519245</td>\n",
       "      <td>-0.030445</td>\n",
       "      <td>0.358564</td>\n",
       "      <td>-0.086921</td>\n",
       "      <td>3.161571</td>\n",
       "      <td>0.590543</td>\n",
       "      <td>-0.161021</td>\n",
       "      <td>-0.106622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0-32</td>\n",
       "      <td>-0.001311</td>\n",
       "      <td>-0.000514</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>-0.001294</td>\n",
       "      <td>-0.000760</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.001459</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.279017</td>\n",
       "      <td>-0.260539</td>\n",
       "      <td>0.519245</td>\n",
       "      <td>-0.030445</td>\n",
       "      <td>0.358564</td>\n",
       "      <td>-0.086921</td>\n",
       "      <td>-0.085898</td>\n",
       "      <td>0.590543</td>\n",
       "      <td>-0.161021</td>\n",
       "      <td>-0.106622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0-34</td>\n",
       "      <td>-0.001311</td>\n",
       "      <td>-0.000514</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>-0.001294</td>\n",
       "      <td>-0.000760</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.001459</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.279017</td>\n",
       "      <td>-0.260539</td>\n",
       "      <td>0.519245</td>\n",
       "      <td>-0.030445</td>\n",
       "      <td>0.358564</td>\n",
       "      <td>-0.086921</td>\n",
       "      <td>-0.085898</td>\n",
       "      <td>0.590543</td>\n",
       "      <td>-0.161021</td>\n",
       "      <td>-0.106622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 541 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   stock_id  time_id row_id  wap1_mean  wap1_amax  wap1_std  wap2_mean  \\\n",
       "0         0        4    0-4   0.250325  -0.543764 -2.429043   0.335747   \n",
       "1         0       32   0-32  -0.001311  -0.000514  0.000281  -0.001294   \n",
       "2         0       34   0-34  -0.001311  -0.000514  0.000281  -0.001294   \n",
       "\n",
       "   wap2_amax  wap2_std  log_return1_realized_volatility  ...  \\\n",
       "0  -0.527097 -2.723074                        -3.421779  ...   \n",
       "1  -0.000760  0.000382                         0.001459  ...   \n",
       "2  -0.000760  0.000382                         0.001459  ...   \n",
       "\n",
       "   bid_ask_spread_mean_0c1  bid_ask_spread_mean_1c1  bid_ask_spread_mean_3c1  \\\n",
       "0                -0.279017                 0.823808                 0.519245   \n",
       "1                -0.279017                -0.260539                 0.519245   \n",
       "2                -0.279017                -0.260539                 0.519245   \n",
       "\n",
       "   bid_ask_spread_mean_4c1  bid_ask_spread_mean_2c1  size_tau2_0c1  \\\n",
       "0                -0.030445                 0.358564      -0.086921   \n",
       "1                -0.030445                 0.358564      -0.086921   \n",
       "2                -0.030445                 0.358564      -0.086921   \n",
       "\n",
       "   size_tau2_1c1  size_tau2_3c1  size_tau2_4c1  size_tau2_2c1  \n",
       "0       3.161571       0.590543      -0.161021      -0.106622  \n",
       "1      -0.085898       0.590543      -0.161021      -0.106622  \n",
       "2      -0.085898       0.590543      -0.161021      -0.106622  \n",
       "\n",
       "[3 rows x 541 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3679c03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:26:20.281635Z",
     "iopub.status.busy": "2021-09-27T11:26:20.280008Z",
     "iopub.status.idle": "2021-09-27T11:26:24.901855Z",
     "shell.execute_reply": "2021-09-27T11:26:24.900955Z"
    },
    "papermill": {
     "duration": 4.717763,
     "end_time": "2021-09-27T11:26:24.901997",
     "exception": false,
     "start_time": "2021-09-27T11:26:20.184234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-27 11:26:20.289396: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-27 11:26:20.292284: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-09-27 11:26:20.335475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-27 11:26:20.336092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2021-09-27 11:26:20.336162: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-09-27 11:26:20.363517: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-09-27 11:26:20.363625: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-09-27 11:26:20.393959: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-09-27 11:26:20.407831: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-09-27 11:26:20.431617: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-09-27 11:26:20.439026: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-09-27 11:26:20.442406: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-09-27 11:26:20.442597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-27 11:26:20.443296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-27 11:26:20.444670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-09-27 11:26:20.445122: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-27 11:26:20.445320: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-27 11:26:20.445495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-27 11:26:20.446058: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2021-09-27 11:26:20.446112: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-09-27 11:26:20.446139: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-09-27 11:26:20.446158: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-09-27 11:26:20.446176: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-09-27 11:26:20.446193: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-09-27 11:26:20.446210: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-09-27 11:26:20.446229: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-09-27 11:26:20.446247: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-09-27 11:26:20.446360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-27 11:26:20.446970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-27 11:26:20.447526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-09-27 11:26:20.448411: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-09-27 11:26:21.889037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-09-27 11:26:21.889093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2021-09-27 11:26:21.889103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2021-09-27 11:26:21.891392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-27 11:26:21.892096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-27 11:26:21.892752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-27 11:26:21.893310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14957 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
      "2021-09-27 11:26:22.361459: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-27 11:26:22.372700: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2000175000 Hz\n",
      "2021-09-27 11:26:22.504930: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-09-27 11:26:23.261649: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n"
     ]
    }
   ],
   "source": [
    "n_folds=5\n",
    "test_preds = np.zeros(len(test))\n",
    "\n",
    "for n_count in range(n_folds):\n",
    "    \n",
    "    model = base_model()\n",
    "    \n",
    "    model.compile(\n",
    "        keras.optimizers.Adam(learning_rate=0.004), \n",
    "        loss=root_mean_squared_per_error\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        features_to_consider.remove('stock_id')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    model.load_weights(f'../input/ng-wwdd/n2_model_{n_count}.hdf5')\n",
    "\n",
    "    scaler=load(open(f'/kaggle/input/ng-wwdd/scaler_{n_count}.pkl', 'rb'))\n",
    "    tt =scaler.transform(test[features_to_consider].values)\n",
    "    test_preds += model.predict([test['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n",
    "       \n",
    "    features_to_consider.append('stock_id')\n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cb4afe2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:26:24.982025Z",
     "iopub.status.busy": "2021-09-27T11:26:24.981262Z",
     "iopub.status.idle": "2021-09-27T11:26:24.983821Z",
     "shell.execute_reply": "2021-09-27T11:26:24.983427Z"
    },
    "papermill": {
     "duration": 0.044112,
     "end_time": "2021-09-27T11:26:24.983930",
     "exception": false,
     "start_time": "2021-09-27T11:26:24.939818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ng_ffgg1=test_preds/n_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c0b289d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:26:25.063840Z",
     "iopub.status.busy": "2021-09-27T11:26:25.063141Z",
     "iopub.status.idle": "2021-09-27T11:26:52.711264Z",
     "shell.execute_reply": "2021-09-27T11:26:52.711784Z"
    },
    "papermill": {
     "duration": 27.690135,
     "end_time": "2021-09-27T11:26:52.711941",
     "exception": false,
     "start_time": "2021-09-27T11:26:25.021806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ../input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f300e833",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:26:52.798063Z",
     "iopub.status.busy": "2021-09-27T11:26:52.797078Z",
     "iopub.status.idle": "2021-09-27T11:26:57.092188Z",
     "shell.execute_reply": "2021-09-27T11:26:57.091692Z"
    },
    "papermill": {
     "duration": 4.342331,
     "end_time": "2021-09-27T11:26:57.092310",
     "exception": false,
     "start_time": "2021-09-27T11:26:52.749979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy.matlib\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "orange_black = [\n",
    "    '#fdc029', '#df861d', '#FF6347', '#aa3d01', '#a30e15', '#800000', '#171820'\n",
    "]\n",
    "plt.rcParams['figure.figsize'] = (16,9)\n",
    "plt.rcParams[\"figure.facecolor\"] = '#FFFACD'\n",
    "plt.rcParams[\"axes.facecolor\"] = '#FFFFE0'\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "plt.rcParams[\"grid.color\"] = orange_black[3]\n",
    "plt.rcParams[\"grid.alpha\"] = 0.5\n",
    "plt.rcParams[\"grid.linestyle\"] = '--'\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d863d16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:26:57.177175Z",
     "iopub.status.busy": "2021-09-27T11:26:57.172167Z",
     "iopub.status.idle": "2021-09-27T11:26:58.146111Z",
     "shell.execute_reply": "2021-09-27T11:26:58.146663Z"
    },
    "papermill": {
     "duration": 1.016351,
     "end_time": "2021-09-27T11:26:58.146848",
     "exception": false,
     "start_time": "2021-09-27T11:26:57.130497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.7s finished\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../input/optiver-realized-volatility-prediction/'\n",
    "\n",
    "\n",
    "def calc_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap3(df):\n",
    "    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap4(df):\n",
    "    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "\n",
    "def log_return(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n",
    "\n",
    "\n",
    "def read_train_test():\n",
    "    \n",
    "    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n",
    "    \n",
    "    \n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    \n",
    "    return test\n",
    "\n",
    "\n",
    "def book_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    \n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    \n",
    "    df['wap_mean'] = (df['wap1'] + df['wap2']) / 2\n",
    "    \n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
    "    \n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "    \n",
    "    df['bas'] = (df[['ask_price1', 'ask_price2']].min(axis = 1)/ df[['bid_price1', 'bid_price2']].max(axis = 1) - 1) \n",
    "    df['h_spread_l1'] = df['ask_price1'] - df['bid_price1']\n",
    "    df['h_spread_l2'] = df['ask_price2'] - df['bid_price2']\n",
    "    df['log_return_bid_price1'] = np.log(df['bid_price1'].pct_change() + 1)\n",
    "    df['log_return_ask_price1'] = np.log(df['ask_price1'].pct_change() + 1)\n",
    "    df['log_return_bid_size1'] = np.log(df['bid_size1'].pct_change() + 1)\n",
    "    df['log_return_ask_size1'] = np.log(df['ask_size1'].pct_change() + 1)\n",
    "    df['log_ask_1_div_bid_1'] = np.log(df['ask_price1'] / df['bid_price1'])\n",
    "    df['log_ask_1_div_bid_1_size'] = np.log(df['ask_size1'] / df['bid_size1'])\n",
    "    df['log_return_bid_price2'] = np.log(df['bid_price2'].pct_change() + 1)\n",
    "    df['log_return_ask_price2'] = np.log(df['ask_price2'].pct_change() + 1)\n",
    "    df['log_return_bid_size2'] = np.log(df['bid_size2'].pct_change() + 1)\n",
    "    df['log_return_ask_size2'] = np.log(df['ask_size2'].pct_change() + 1)\n",
    "    df['log_ask_2_div_bid_2'] = np.log(df['ask_price2'] / df['bid_price2'])\n",
    "    df['log_ask_2_div_bid_2_size'] = np.log(df['ask_size2'] / df['bid_size2'])\n",
    "    \n",
    "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
    "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    create_feature_dict = {\n",
    "        'wap1': [np.mean,np.max,np.std],\n",
    "        'wap2': [np.mean,np.max,np.std],\n",
    "\n",
    "        'log_return1': [realized_volatility,np.sum],\n",
    "        'log_return2': [realized_volatility,np.sum], \n",
    "        'log_return_bid_price1':[realized_volatility,np.sum], \n",
    "        'log_return_ask_price1':[realized_volatility,np.sum],\n",
    "        'log_return_bid_size1':[realized_volatility,np.sum], \n",
    "        'log_return_ask_size1':[realized_volatility,np.sum], \n",
    "        'log_ask_1_div_bid_1':[realized_volatility,np.sum], \n",
    "        'log_ask_1_div_bid_1_size':[realized_volatility,np.sum], \n",
    "        'log_return_bid_price2':[realized_volatility,np.sum], \n",
    "        'log_return_ask_price2':[realized_volatility,np.sum], \n",
    "        'log_return_bid_size2':[realized_volatility,np.sum], \n",
    "        'log_return_ask_size2':[realized_volatility,np.sum], \n",
    "        'log_ask_2_div_bid_2':[realized_volatility,np.sum], \n",
    "        'log_ask_2_div_bid_2_size':[realized_volatility,np.sum], \n",
    " \n",
    "        'wap_balance': [np.mean,np.max,np.std,np.sum], \n",
    "        'wap_mean': [np.mean,np.max,np.std,np.sum],\n",
    "        'price_spread':[np.mean,np.max,np.std,np.sum],\n",
    "        'bid_spread':[np.mean,np.max,np.std,np.sum],\n",
    "        'ask_spread':[np.mean,np.max,np.std,np.sum],\n",
    "        'total_volume':[np.mean,np.max,np.std,np.sum],\n",
    "        'volume_imbalance':[np.mean,np.max,np.std,np.sum],\n",
    "        'bas':[np.mean,np.max,np.std,np.sum],\n",
    "        'h_spread_l1':[np.mean,np.max,np.std,np.sum],\n",
    "        'h_spread_l2':[np.mean,np.max,np.std,np.sum],\n",
    "        'price_spread2':[np.mean,np.max,np.std,np.sum],\n",
    "        \"bid_ask_spread\":[np.mean,np.max,np.std,np.sum]\n",
    "    }\n",
    "    \n",
    "    \n",
    "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
    "        \n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n",
    "        \n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        \n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    \n",
    "    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n",
    "    \n",
    "    \n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "\n",
    "    \n",
    "    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "def trade_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    \n",
    "    \n",
    "    create_feature_dict = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n",
    "        'order_count':[np.mean,np.sum,np.max],\n",
    "    }\n",
    "    \n",
    "    \n",
    "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
    "        \n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n",
    "        \n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        \n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "\n",
    "    \n",
    "    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n",
    "    \n",
    "    def tendency(price, vol):    \n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff/price[1:])*100\n",
    "        power = np.sum(val*vol[1:])\n",
    "        return(power)\n",
    "    \n",
    "    lis = []\n",
    "    for n_time_id in df['time_id'].unique():\n",
    "        df_id = df[df['time_id'] == n_time_id]        \n",
    "        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n",
    "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
    "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
    "        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n",
    "        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n",
    "        \n",
    "        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n",
    "        energy = np.mean(df_id['price'].values**2)\n",
    "        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n",
    "        \n",
    "        \n",
    "        \n",
    "        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n",
    "        energy_v = np.sum(df_id['size'].values**2)\n",
    "        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n",
    "        \n",
    "        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n",
    "                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n",
    "    \n",
    "    df_lr = pd.DataFrame(lis)\n",
    "        \n",
    "   \n",
    "    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
    "    \n",
    "    \n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "\n",
    "    \n",
    "    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200','time_id'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "\n",
    "def get_time_stock(df):\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n",
    "\n",
    "\n",
    "    \n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    \n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    \n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    \n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "    \n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
    "    return df\n",
    "    \n",
    "\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    \n",
    "    \n",
    "    def for_joblib(stock_id):\n",
    "        \n",
    "        if is_train:\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        \n",
    "        else:\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    "        \n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
    "        \n",
    "        \n",
    "        return df_tmp\n",
    "    \n",
    "    \n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "    \n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "test = read_train_test()\n",
    "\n",
    "\n",
    "test_stock_ids = test['stock_id'].unique()\n",
    "\n",
    "test_ = preprocessor(test_stock_ids, is_train = False)\n",
    "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
    "\n",
    "\n",
    "test = get_time_stock(test)\n",
    "\n",
    "test['size_tau'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique'] )\n",
    "test['size_tau_400'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_400'] )\n",
    "test['size_tau_300'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_300'] )\n",
    "test['size_tau_200'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_200'] )\n",
    "test['size_tau2'] = np.sqrt( 1/ test['trade_order_count_sum'] )\n",
    "test['size_tau2_400'] = np.sqrt( 0.33/ test['trade_order_count_sum'] )\n",
    "test['size_tau2_300'] = np.sqrt( 0.5/ test['trade_order_count_sum'] )\n",
    "test['size_tau2_200'] = np.sqrt( 0.66/ test['trade_order_count_sum'] )\n",
    "\n",
    "\n",
    "test['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc7b08e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:26:58.234922Z",
     "iopub.status.busy": "2021-09-27T11:26:58.234133Z",
     "iopub.status.idle": "2021-09-27T11:27:04.756565Z",
     "shell.execute_reply": "2021-09-27T11:27:04.755581Z"
    },
    "papermill": {
     "duration": 6.569745,
     "end_time": "2021-09-27T11:27:04.756704",
     "exception": false,
     "start_time": "2021-09-27T11:26:58.186959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 11, 22, 50, 55, 56, 62, 73, 76, 78, 84, 87, 96, 101, 112, 116, 122, 124, 126]\n",
      "[0, 4, 5, 10, 15, 16, 17, 23, 26, 28, 29, 36, 42, 44, 48, 53, 66, 69, 72, 85, 94, 95, 100, 102, 109, 111, 113, 115, 118, 120]\n",
      "[3, 6, 9, 18, 61, 63, 86, 97]\n",
      "[27, 31, 33, 37, 38, 40, 58, 59, 60, 74, 75, 77, 82, 83, 88, 89, 90, 98, 99, 110]\n",
      "[2, 7, 13, 14, 19, 20, 21, 30, 32, 34, 35, 39, 41, 43, 46, 47, 51, 52, 64, 67, 68, 70, 93, 103, 104, 105, 107, 108, 114, 119, 123, 125]\n",
      "[81]\n",
      "[8, 80]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "corr = train_p.corr()\n",
    "\n",
    "ids = corr.index\n",
    "\n",
    "label=[1, 0, 4, 2, 1, 1, 2, 4, 6, 2, 1, 0, 4, 4, 1, 1, 1, 2, 4, 4, 4, 0, 1, 1, 3, 1, 1, 4, 3, 4, 3, 4, 4, 1, 3, 3, 4,\n",
    "       3, 4, 1, 4, 1, 4, 4, 1, 0, 4, 4, 1, 0, 0, 3, 3, 3, 2, 0, 2, 4, 1, 4, 4, 1, 4, 1, 0, 3, 3, 0, 3, 0, 6, 5, 3, 3,\n",
    "       0, 1, 2, 0, 3, 3, 3, 4, 1, 1, 0, 2, 3, 3, 1, 0, 1, 4, 4, 4, 4, 4, 1, 3, 1, 0, 1, 4, 1, 0, 1, 4, 1, 0, 4, 0, 4,\n",
    "       0]\n",
    "\n",
    "l = []\n",
    "for n in range(7):\n",
    "    l.append ( [ (x-1) for x in ( (ids+1)*(label==n*np.ones(len(label)))) if x > 0] )\n",
    "    \n",
    "\n",
    "\n",
    "matTest = []\n",
    "\n",
    "n = 0\n",
    "for ind in l:\n",
    "    print(ind)\n",
    "    \n",
    "    newDf = test.loc[test['stock_id'].isin(ind) ]    \n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    matTest.append ( newDf )\n",
    "    \n",
    "    n+=1\n",
    "\n",
    "mat2 = pd.concat(matTest).reset_index()\n",
    "mat1=pd.read_csv('/kaggle/input/tg-wwdd/mat1.csv')\n",
    "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n",
    "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "mat2.reset_index(inplace=True)\n",
    "nnn = ['time_id',\n",
    "     'log_return1_realized_volatility_0c1',\n",
    "     'log_return1_realized_volatility_1c1',     \n",
    "     'log_return1_realized_volatility_3c1',\n",
    "     'log_return1_realized_volatility_4c1',     \n",
    "     'log_return1_realized_volatility_2c1',\n",
    "     'total_volume_sum_0c1',\n",
    "     'total_volume_sum_1c1', \n",
    "     'total_volume_sum_3c1',\n",
    "     'total_volume_sum_4c1', \n",
    "     'total_volume_sum_2c1',\n",
    "     'trade_size_sum_0c1',\n",
    "     'trade_size_sum_1c1', \n",
    "     'trade_size_sum_3c1',\n",
    "     'trade_size_sum_4c1', \n",
    "     'trade_size_sum_2c1',\n",
    "     'trade_order_count_sum_0c1',\n",
    "     'trade_order_count_sum_1c1',\n",
    "     'trade_order_count_sum_3c1',\n",
    "     'trade_order_count_sum_4c1',\n",
    "     'trade_order_count_sum_2c1',      \n",
    "     'price_spread_sum_0c1',\n",
    "     'price_spread_sum_1c1',\n",
    "     'price_spread_sum_3c1',\n",
    "     'price_spread_sum_4c1',\n",
    "     'price_spread_sum_2c1',   \n",
    "     'bid_spread_sum_0c1',\n",
    "     'bid_spread_sum_1c1',\n",
    "     'bid_spread_sum_3c1',\n",
    "     'bid_spread_sum_4c1',\n",
    "     'bid_spread_sum_2c1',       \n",
    "     'ask_spread_sum_0c1',\n",
    "     'ask_spread_sum_1c1',\n",
    "     'ask_spread_sum_3c1',\n",
    "     'ask_spread_sum_4c1',\n",
    "     'ask_spread_sum_2c1',   \n",
    "     'volume_imbalance_sum_0c1',\n",
    "     'volume_imbalance_sum_1c1',\n",
    "     'volume_imbalance_sum_3c1',\n",
    "     'volume_imbalance_sum_4c1',\n",
    "     'volume_imbalance_sum_2c1',       \n",
    "     'bid_ask_spread_sum_0c1',\n",
    "     'bid_ask_spread_sum_1c1',\n",
    "     'bid_ask_spread_sum_3c1',\n",
    "     'bid_ask_spread_sum_4c1',\n",
    "     'bid_ask_spread_sum_2c1',\n",
    "     'size_tau2_0c1',\n",
    "     'size_tau2_1c1',\n",
    "     'size_tau2_3c1',\n",
    "     'size_tau2_4c1',\n",
    "     'size_tau2_2c1'] \n",
    "test = pd.merge(test,mat2[nnn],how='left',on='time_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5734042c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:27:04.841568Z",
     "iopub.status.busy": "2021-09-27T11:27:04.841017Z",
     "iopub.status.idle": "2021-09-27T11:27:04.931773Z",
     "shell.execute_reply": "2021-09-27T11:27:04.932185Z"
    },
    "papermill": {
     "duration": 0.134931,
     "end_time": "2021-09-27T11:27:04.932315",
     "exception": false,
     "start_time": "2021-09-27T11:27:04.797384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = [col for col in test.columns.tolist() if col not in ['time_id','row_id']]\n",
    "f_mean = np.load('/kaggle/input/tg-wwdd/f_mean.npy')\n",
    "for i in range(len(features)):\n",
    "    test[features[i]] = test[features[i]].fillna(f_mean[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f91534c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:27:05.024907Z",
     "iopub.status.busy": "2021-09-27T11:27:05.014125Z",
     "iopub.status.idle": "2021-09-27T11:27:09.001901Z",
     "shell.execute_reply": "2021-09-27T11:27:09.001438Z"
    },
    "papermill": {
     "duration": 4.030432,
     "end_time": "2021-09-27T11:27:09.002028",
     "exception": false,
     "start_time": "2021-09-27T11:27:04.971596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test=test.copy()\n",
    "X_test.drop(['time_id','row_id'], axis=1,inplace=True)\n",
    "def rmspe(y_true, y_pred):\n",
    "    \n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "class RMSPE(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"rmspe\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        \n",
    "        return np.sqrt(np.mean(np.square((y_true - y_score) / y_true)))\n",
    "    \n",
    "\n",
    "\n",
    "def RMSPELoss(y_pred, y_true):\n",
    "    return torch.sqrt(torch.mean( ((y_true - y_pred) / y_true) ** 2 )).clone()\n",
    "from pickle import load,dump\n",
    "\n",
    "for col in features:\n",
    "    if  col == 'stock_id':\n",
    "        l_enc=load(open('/kaggle/input/tg-wwdd/l_enc.pkl', 'rb'))\n",
    "        X_test[col] = l_enc.transform(X_test[col].values)\n",
    "    else:\n",
    "        scaler=load(open(f'/kaggle/input/tg-wwdd/scaler_{col}.pkl', 'rb'))\n",
    "        X_test[col] = scaler.transform(X_test[col].values.reshape(-1, 1))\n",
    "cat_idxs_df=pd.read_csv('/kaggle/input/tg-wwdd/cat_idxs_df.csv')\n",
    "cat_dims_df=pd.read_csv('/kaggle/input/tg-wwdd/cat_dims_df.csv')\n",
    "cat_idxs=cat_idxs_df['cat_idxs']\n",
    "cat_dims=cat_dims_df['cat_dims']\n",
    "tabnet_params = dict(\n",
    "    cat_idxs=cat_idxs,\n",
    "    cat_dims=cat_dims,\n",
    "    cat_emb_dim=1,\n",
    "    n_d = 13,\n",
    "    n_a = 13,\n",
    "    n_steps = 1,\n",
    "    gamma = 2,\n",
    "    n_independent = 2,\n",
    "    n_shared = 2,\n",
    "    lambda_sparse = 0,\n",
    "    optimizer_fn = Adam,\n",
    "    optimizer_params = dict(lr = (2e-2)),\n",
    "    mask_type = \"entmax\",\n",
    "    scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False),\n",
    "    scheduler_fn = CosineAnnealingWarmRestarts,\n",
    "    seed = 60,\n",
    "    verbose = 10\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "522769ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:27:09.106580Z",
     "iopub.status.busy": "2021-09-27T11:27:09.101657Z",
     "iopub.status.idle": "2021-09-27T11:27:09.109260Z",
     "shell.execute_reply": "2021-09-27T11:27:09.109684Z"
    },
    "papermill": {
     "duration": 0.068644,
     "end_time": "2021-09-27T11:27:09.109809",
     "exception": false,
     "start_time": "2021-09-27T11:27:09.041165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>wap1_mean</th>\n",
       "      <th>wap1_amax</th>\n",
       "      <th>wap1_std</th>\n",
       "      <th>wap2_mean</th>\n",
       "      <th>wap2_amax</th>\n",
       "      <th>wap2_std</th>\n",
       "      <th>log_return1_realized_volatility</th>\n",
       "      <th>log_return1_sum</th>\n",
       "      <th>log_return2_realized_volatility</th>\n",
       "      <th>...</th>\n",
       "      <th>bid_ask_spread_sum_0c1</th>\n",
       "      <th>bid_ask_spread_sum_1c1</th>\n",
       "      <th>bid_ask_spread_sum_3c1</th>\n",
       "      <th>bid_ask_spread_sum_4c1</th>\n",
       "      <th>bid_ask_spread_sum_2c1</th>\n",
       "      <th>size_tau2_0c1</th>\n",
       "      <th>size_tau2_1c1</th>\n",
       "      <th>size_tau2_3c1</th>\n",
       "      <th>size_tau2_4c1</th>\n",
       "      <th>size_tau2_2c1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.191565e-01</td>\n",
       "      <td>-4.375152e-01</td>\n",
       "      <td>-8.941496e-01</td>\n",
       "      <td>1.623597e-01</td>\n",
       "      <td>-4.365765e-01</td>\n",
       "      <td>-9.351345e-01</td>\n",
       "      <td>-1.098426e+00</td>\n",
       "      <td>8.241560e-02</td>\n",
       "      <td>-1.105520e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.408291e-13</td>\n",
       "      <td>-2.253557e+00</td>\n",
       "      <td>1.374878e-13</td>\n",
       "      <td>6.954657e-14</td>\n",
       "      <td>2.642473e-13</td>\n",
       "      <td>-2.844108e-13</td>\n",
       "      <td>2.003125e+01</td>\n",
       "      <td>-5.454171e-13</td>\n",
       "      <td>-1.019213e-12</td>\n",
       "      <td>-1.264623e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-2.042257e-12</td>\n",
       "      <td>1.225712e-12</td>\n",
       "      <td>3.505232e-15</td>\n",
       "      <td>9.872247e-13</td>\n",
       "      <td>-5.769340e-14</td>\n",
       "      <td>9.161438e-15</td>\n",
       "      <td>9.433118e-15</td>\n",
       "      <td>4.194278e-17</td>\n",
       "      <td>9.664863e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.408291e-13</td>\n",
       "      <td>-3.196314e-13</td>\n",
       "      <td>1.374878e-13</td>\n",
       "      <td>6.954657e-14</td>\n",
       "      <td>2.642473e-13</td>\n",
       "      <td>-2.844108e-13</td>\n",
       "      <td>6.856557e-13</td>\n",
       "      <td>-5.454171e-13</td>\n",
       "      <td>-1.019213e-12</td>\n",
       "      <td>-1.264623e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-2.042257e-12</td>\n",
       "      <td>1.225712e-12</td>\n",
       "      <td>3.505232e-15</td>\n",
       "      <td>9.872247e-13</td>\n",
       "      <td>-5.769340e-14</td>\n",
       "      <td>9.161438e-15</td>\n",
       "      <td>9.433118e-15</td>\n",
       "      <td>4.194278e-17</td>\n",
       "      <td>9.664863e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.408291e-13</td>\n",
       "      <td>-3.196314e-13</td>\n",
       "      <td>1.374878e-13</td>\n",
       "      <td>6.954657e-14</td>\n",
       "      <td>2.642473e-13</td>\n",
       "      <td>-2.844108e-13</td>\n",
       "      <td>6.856557e-13</td>\n",
       "      <td>-5.454171e-13</td>\n",
       "      <td>-1.019213e-12</td>\n",
       "      <td>-1.264623e-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 539 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   stock_id     wap1_mean     wap1_amax      wap1_std     wap2_mean  \\\n",
       "0         0  1.191565e-01 -4.375152e-01 -8.941496e-01  1.623597e-01   \n",
       "1         0 -2.042257e-12  1.225712e-12  3.505232e-15  9.872247e-13   \n",
       "2         0 -2.042257e-12  1.225712e-12  3.505232e-15  9.872247e-13   \n",
       "\n",
       "      wap2_amax      wap2_std  log_return1_realized_volatility  \\\n",
       "0 -4.365765e-01 -9.351345e-01                    -1.098426e+00   \n",
       "1 -5.769340e-14  9.161438e-15                     9.433118e-15   \n",
       "2 -5.769340e-14  9.161438e-15                     9.433118e-15   \n",
       "\n",
       "   log_return1_sum  log_return2_realized_volatility  ...  \\\n",
       "0     8.241560e-02                    -1.105520e+00  ...   \n",
       "1     4.194278e-17                     9.664863e-15  ...   \n",
       "2     4.194278e-17                     9.664863e-15  ...   \n",
       "\n",
       "   bid_ask_spread_sum_0c1  bid_ask_spread_sum_1c1  bid_ask_spread_sum_3c1  \\\n",
       "0            1.408291e-13           -2.253557e+00            1.374878e-13   \n",
       "1            1.408291e-13           -3.196314e-13            1.374878e-13   \n",
       "2            1.408291e-13           -3.196314e-13            1.374878e-13   \n",
       "\n",
       "   bid_ask_spread_sum_4c1  bid_ask_spread_sum_2c1  size_tau2_0c1  \\\n",
       "0            6.954657e-14            2.642473e-13  -2.844108e-13   \n",
       "1            6.954657e-14            2.642473e-13  -2.844108e-13   \n",
       "2            6.954657e-14            2.642473e-13  -2.844108e-13   \n",
       "\n",
       "   size_tau2_1c1  size_tau2_3c1  size_tau2_4c1  size_tau2_2c1  \n",
       "0   2.003125e+01  -5.454171e-13  -1.019213e-12  -1.264623e-13  \n",
       "1   6.856557e-13  -5.454171e-13  -1.019213e-12  -1.264623e-13  \n",
       "2   6.856557e-13  -5.454171e-13  -1.019213e-12  -1.264623e-13  \n",
       "\n",
       "[3 rows x 539 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddc78b4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:27:09.197372Z",
     "iopub.status.busy": "2021-09-27T11:27:09.196857Z",
     "iopub.status.idle": "2021-09-27T11:27:09.232206Z",
     "shell.execute_reply": "2021-09-27T11:27:09.231789Z"
    },
    "papermill": {
     "duration": 0.079769,
     "end_time": "2021-09-27T11:27:09.232309",
     "exception": false,
     "start_time": "2021-09-27T11:27:09.152540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "pathB = \"../input/tg-wwdd/\"\n",
    "modelpath = [os.path.join(pathB,s) for s in os.listdir(pathB) if (\"zip\" in s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af752806",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:27:09.315715Z",
     "iopub.status.busy": "2021-09-27T11:27:09.315032Z",
     "iopub.status.idle": "2021-09-27T11:27:15.121017Z",
     "shell.execute_reply": "2021-09-27T11:27:15.121895Z"
    },
    "papermill": {
     "duration": 5.850228,
     "end_time": "2021-09-27T11:27:15.122092",
     "exception": false,
     "start_time": "2021-09-27T11:27:09.271864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n"
     ]
    }
   ],
   "source": [
    "test_predictions=[]\n",
    "for path in modelpath:\n",
    "    clf =  TabNetRegressor(**tabnet_params)\n",
    "    clf.load_model(path)\n",
    "    test_predictions.append(clf.predict(X_test.values).squeeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89e92783",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:27:15.208780Z",
     "iopub.status.busy": "2021-09-27T11:27:15.208107Z",
     "iopub.status.idle": "2021-09-27T11:27:15.210436Z",
     "shell.execute_reply": "2021-09-27T11:27:15.210837Z"
    },
    "papermill": {
     "duration": 0.047076,
     "end_time": "2021-09-27T11:27:15.210999",
     "exception": false,
     "start_time": "2021-09-27T11:27:15.163923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tg_ffgg1 =  np.mean(test_predictions,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b00abcdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:27:15.299476Z",
     "iopub.status.busy": "2021-09-27T11:27:15.298101Z",
     "iopub.status.idle": "2021-09-27T11:27:15.300167Z",
     "shell.execute_reply": "2021-09-27T11:27:15.300582Z"
    },
    "papermill": {
     "duration": 0.048907,
     "end_time": "2021-09-27T11:27:15.300700",
     "exception": false,
     "start_time": "2021-09-27T11:27:15.251793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import numpy.matlib\n",
    "\n",
    "target_name = 'target'\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56a22d9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:27:15.386268Z",
     "iopub.status.busy": "2021-09-27T11:27:15.385379Z",
     "iopub.status.idle": "2021-09-27T11:27:15.797171Z",
     "shell.execute_reply": "2021-09-27T11:27:15.797816Z"
    },
    "papermill": {
     "duration": 0.456022,
     "end_time": "2021-09-27T11:27:15.798008",
     "exception": false,
     "start_time": "2021-09-27T11:27:15.341986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../input/optiver-realized-volatility-prediction/'\n",
    "\n",
    "\n",
    "def calc_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap3(df):\n",
    "    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap4(df):\n",
    "    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "\n",
    "def log_return(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n",
    "\n",
    "\n",
    "def read_test():\n",
    "    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n",
    "    \n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    return test\n",
    "\n",
    "\n",
    "def book_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    df['wap3'] = calc_wap3(df)\n",
    "    df['wap4'] = calc_wap4(df)\n",
    "    \n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n",
    "    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n",
    "    \n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
    "    \n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "    \n",
    "    \n",
    "    create_feature_dict = {\n",
    "        'wap1': [np.sum, np.std],\n",
    "        'wap2': [np.sum, np.std],\n",
    "        'wap3': [np.sum, np.std],\n",
    "        'wap4': [np.sum, np.std],\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "        'wap_balance': [np.sum, np.max],\n",
    "        'price_spread':[np.sum, np.max],\n",
    "        'price_spread2':[np.sum, np.max],\n",
    "        'bid_spread':[np.sum, np.max],\n",
    "        'ask_spread':[np.sum, np.max],\n",
    "        'total_volume':[np.sum, np.max],\n",
    "        'volume_imbalance':[np.sum, np.max],\n",
    "        \"bid_ask_spread\":[np.sum,  np.max],\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "    }\n",
    "    \n",
    "    \n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        \n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        \n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        \n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    \n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "\n",
    "    \n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    \n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "\n",
    "def trade_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    df['amount']=df['price']*df['size']\n",
    "    \n",
    "    create_feature_dict = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum, np.max, np.min],\n",
    "        'order_count':[np.sum,np.max],\n",
    "        'amount':[np.sum,np.max,np.min],\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum],\n",
    "        'order_count':[np.sum],\n",
    "    }\n",
    "    \n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        \n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        \n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        \n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "\n",
    "    \n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "    \n",
    "    def tendency(price, vol):    \n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff/price[1:])*100\n",
    "        power = np.sum(val*vol[1:])\n",
    "        return(power)\n",
    "    \n",
    "    lis = []\n",
    "    for n_time_id in df['time_id'].unique():\n",
    "        df_id = df[df['time_id'] == n_time_id]        \n",
    "        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n",
    "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
    "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
    "        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n",
    "        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n",
    "        \n",
    "        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n",
    "        energy = np.mean(df_id['price'].values**2)\n",
    "        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n",
    "        \n",
    "        \n",
    "        \n",
    "        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n",
    "        energy_v = np.sum(df_id['size'].values**2)\n",
    "        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n",
    "        \n",
    "        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n",
    "                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n",
    "    \n",
    "    df_lr = pd.DataFrame(lis)\n",
    "        \n",
    "   \n",
    "    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
    "    \n",
    "    \n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    \n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "\n",
    "def get_time_stock(df):\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n",
    "\n",
    "\n",
    "    \n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    \n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    \n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    \n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "    \n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
    "    return df\n",
    "    \n",
    "\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    \n",
    "    \n",
    "    def for_joblib(stock_id):\n",
    "        \n",
    "        if is_train:\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        \n",
    "        else:\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    "        \n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
    "        \n",
    "        \n",
    "        return df_tmp\n",
    "    \n",
    "    \n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "    \n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "test = read_test()\n",
    "\n",
    "\n",
    "test_stock_ids = test['stock_id'].unique()\n",
    "\n",
    "test_ = preprocessor(test_stock_ids, is_train = False)\n",
    "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
    "\n",
    "\n",
    "test = get_time_stock(test)\n",
    "\n",
    "test['size_tau'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique'] )\n",
    "test['size_tau_400'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_400'] )\n",
    "test['size_tau_300'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_300'] )\n",
    "test['size_tau_200'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_200'] )\n",
    "test['size_tau2'] = np.sqrt( 1/ test['trade_order_count_sum'] )\n",
    "test['size_tau2_400'] = np.sqrt( 0.33/ test['trade_order_count_sum'] )\n",
    "test['size_tau2_300'] = np.sqrt( 0.5/ test['trade_order_count_sum'] )\n",
    "test['size_tau2_200'] = np.sqrt( 0.66/ test['trade_order_count_sum'] )\n",
    "\n",
    "\n",
    "test['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4db568a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:27:15.901432Z",
     "iopub.status.busy": "2021-09-27T11:27:15.900890Z",
     "iopub.status.idle": "2021-09-27T11:27:18.915919Z",
     "shell.execute_reply": "2021-09-27T11:27:18.914950Z"
    },
    "papermill": {
     "duration": 3.073556,
     "end_time": "2021-09-27T11:27:18.916058",
     "exception": false,
     "start_time": "2021-09-27T11:27:15.842502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 11, 22, 50, 55, 56, 62, 73, 76, 78, 84, 87, 96, 101, 112, 116, 122, 124, 126]\n",
      "[0, 4, 5, 10, 15, 16, 17, 23, 26, 28, 29, 36, 42, 44, 48, 53, 66, 69, 72, 85, 94, 95, 100, 102, 109, 111, 113, 115, 118, 120]\n",
      "[3, 6, 9, 18, 61, 63, 86, 97]\n",
      "[27, 31, 33, 37, 38, 40, 58, 59, 60, 74, 75, 77, 82, 83, 88, 89, 90, 98, 99, 110]\n",
      "[2, 7, 13, 14, 19, 20, 21, 30, 32, 34, 35, 39, 41, 43, 46, 47, 51, 52, 64, 67, 68, 70, 93, 103, 104, 105, 107, 108, 114, 119, 123, 125]\n",
      "[81]\n",
      "[8, 80]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "corr = train_p.corr()\n",
    "\n",
    "ids = corr.index\n",
    "\n",
    "label=[1, 0, 4, 2, 1, 1, 2, 4, 6, 2, 1, 0, 4, 4, 1, 1, 1, 2, 4, 4, 4, 0, 1, 1, 3, 1, 1, 4, 3, 4, 3, 4, 4, 1, 3, 3, 4,\n",
    "       3, 4, 1, 4, 1, 4, 4, 1, 0, 4, 4, 1, 0, 0, 3, 3, 3, 2, 0, 2, 4, 1, 4, 4, 1, 4, 1, 0, 3, 3, 0, 3, 0, 6, 5, 3, 3,\n",
    "       0, 1, 2, 0, 3, 3, 3, 4, 1, 1, 0, 2, 3, 3, 1, 0, 1, 4, 4, 4, 4, 4, 1, 3, 1, 0, 1, 4, 1, 0, 1, 4, 1, 0, 4, 0, 4,\n",
    "       0]\n",
    "\n",
    "l = []\n",
    "for n in range(7):\n",
    "    l.append ( [ (x-1) for x in ( (ids+1)*(label==n*np.ones(len(label)))) if x > 0] )\n",
    "    \n",
    "matTest = []\n",
    "\n",
    "n = 0\n",
    "for ind in l:\n",
    "    print(ind)\n",
    "    newDf = test.loc[test['stock_id'].isin(ind) ]    \n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    matTest.append ( newDf )\n",
    "    \n",
    "    n+=1  \n",
    "mat2 = pd.concat(matTest).reset_index()\n",
    "mat1=pd.read_csv('/kaggle/input/lg-wwdd/mat1.csv')\n",
    "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n",
    "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "mat2.reset_index(inplace=True)\n",
    "nnn = ['time_id',\n",
    "     'log_return1_realized_volatility_0c1',\n",
    "     'log_return1_realized_volatility_1c1',     \n",
    "     'log_return1_realized_volatility_3c1',\n",
    "     'log_return1_realized_volatility_4c1',     \n",
    "     'log_return1_realized_volatility_2c1',\n",
    "     'total_volume_sum_0c1',\n",
    "     'total_volume_sum_1c1', \n",
    "     'total_volume_sum_3c1',\n",
    "     'total_volume_sum_4c1', \n",
    "     'total_volume_sum_2c1',\n",
    "     'trade_size_sum_0c1',\n",
    "     'trade_size_sum_1c1', \n",
    "     'trade_size_sum_3c1',\n",
    "     'trade_size_sum_4c1', \n",
    "     'trade_size_sum_2c1',\n",
    "     'trade_order_count_sum_0c1',\n",
    "     'trade_order_count_sum_1c1',\n",
    "     'trade_order_count_sum_3c1',\n",
    "     'trade_order_count_sum_4c1',\n",
    "     'trade_order_count_sum_2c1',      \n",
    "     'price_spread_sum_0c1',\n",
    "     'price_spread_sum_1c1',\n",
    "     'price_spread_sum_3c1',\n",
    "     'price_spread_sum_4c1',\n",
    "     'price_spread_sum_2c1',   \n",
    "     'bid_spread_sum_0c1',\n",
    "     'bid_spread_sum_1c1',\n",
    "     'bid_spread_sum_3c1',\n",
    "     'bid_spread_sum_4c1',\n",
    "     'bid_spread_sum_2c1',       \n",
    "     'ask_spread_sum_0c1',\n",
    "     'ask_spread_sum_1c1',\n",
    "     'ask_spread_sum_3c1',\n",
    "     'ask_spread_sum_4c1',\n",
    "     'ask_spread_sum_2c1',   \n",
    "     'volume_imbalance_sum_0c1',\n",
    "     'volume_imbalance_sum_1c1',\n",
    "     'volume_imbalance_sum_3c1',\n",
    "     'volume_imbalance_sum_4c1',\n",
    "     'volume_imbalance_sum_2c1',       \n",
    "     'bid_ask_spread_sum_0c1',\n",
    "     'bid_ask_spread_sum_1c1',\n",
    "     'bid_ask_spread_sum_3c1',\n",
    "     'bid_ask_spread_sum_4c1',\n",
    "     'bid_ask_spread_sum_2c1',\n",
    "     'size_tau2_0c1',\n",
    "     'size_tau2_1c1',\n",
    "     'size_tau2_3c1',\n",
    "     'size_tau2_4c1',\n",
    "     'size_tau2_2c1'] \n",
    "test = pd.merge(test,mat2[nnn],how='left',on='time_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1540de43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:27:19.025781Z",
     "iopub.status.busy": "2021-09-27T11:27:19.024491Z",
     "iopub.status.idle": "2021-09-27T11:27:19.028483Z",
     "shell.execute_reply": "2021-09-27T11:27:19.028883Z"
    },
    "papermill": {
     "duration": 0.066962,
     "end_time": "2021-09-27T11:27:19.029008",
     "exception": false,
     "start_time": "2021-09-27T11:27:18.962046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>row_id</th>\n",
       "      <th>wap1_sum</th>\n",
       "      <th>wap1_std</th>\n",
       "      <th>wap2_sum</th>\n",
       "      <th>wap2_std</th>\n",
       "      <th>wap3_sum</th>\n",
       "      <th>wap3_std</th>\n",
       "      <th>wap4_sum</th>\n",
       "      <th>...</th>\n",
       "      <th>bid_ask_spread_sum_0c1</th>\n",
       "      <th>bid_ask_spread_sum_1c1</th>\n",
       "      <th>bid_ask_spread_sum_3c1</th>\n",
       "      <th>bid_ask_spread_sum_4c1</th>\n",
       "      <th>bid_ask_spread_sum_2c1</th>\n",
       "      <th>size_tau2_0c1</th>\n",
       "      <th>size_tau2_1c1</th>\n",
       "      <th>size_tau2_3c1</th>\n",
       "      <th>size_tau2_4c1</th>\n",
       "      <th>size_tau2_2c1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0-4</td>\n",
       "      <td>3.001215</td>\n",
       "      <td>0.00017</td>\n",
       "      <td>3.00165</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>3.000752</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>2.999481</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001524</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.301511</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0-32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0-34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 247 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   stock_id  time_id row_id  wap1_sum  wap1_std  wap2_sum  wap2_std  wap3_sum  \\\n",
       "0         0        4    0-4  3.001215   0.00017   3.00165  0.000153  3.000752   \n",
       "1         0       32   0-32       NaN       NaN       NaN       NaN       NaN   \n",
       "2         0       34   0-34       NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "   wap3_std  wap4_sum  ...  bid_ask_spread_sum_0c1  bid_ask_spread_sum_1c1  \\\n",
       "0  0.000142  2.999481  ...                     NaN                0.001524   \n",
       "1       NaN       NaN  ...                     NaN                     NaN   \n",
       "2       NaN       NaN  ...                     NaN                     NaN   \n",
       "\n",
       "   bid_ask_spread_sum_3c1  bid_ask_spread_sum_4c1  bid_ask_spread_sum_2c1  \\\n",
       "0                     NaN                     NaN                     NaN   \n",
       "1                     NaN                     NaN                     NaN   \n",
       "2                     NaN                     NaN                     NaN   \n",
       "\n",
       "   size_tau2_0c1  size_tau2_1c1  size_tau2_3c1  size_tau2_4c1  size_tau2_2c1  \n",
       "0            NaN       0.301511            NaN            NaN            NaN  \n",
       "1            NaN            NaN            NaN            NaN            NaN  \n",
       "2            NaN            NaN            NaN            NaN            NaN  \n",
       "\n",
       "[3 rows x 247 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "574d2c99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:27:19.119590Z",
     "iopub.status.busy": "2021-09-27T11:27:19.118877Z",
     "iopub.status.idle": "2021-09-27T11:27:21.093667Z",
     "shell.execute_reply": "2021-09-27T11:27:21.093157Z"
    },
    "papermill": {
     "duration": 2.021354,
     "end_time": "2021-09-27T11:27:21.093807",
     "exception": false,
     "start_time": "2021-09-27T11:27:19.072453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import glob\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "x_test = test.drop(['row_id', 'time_id'], axis = 1)\n",
    "x_test['stock_id'] = x_test['stock_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e6d9b5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:27:21.186051Z",
     "iopub.status.busy": "2021-09-27T11:27:21.185533Z",
     "iopub.status.idle": "2021-09-27T11:27:22.016395Z",
     "shell.execute_reply": "2021-09-27T11:27:22.015877Z"
    },
    "papermill": {
     "duration": 0.879246,
     "end_time": "2021-09-27T11:27:22.016528",
     "exception": false,
     "start_time": "2021-09-27T11:27:21.137282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_DIR =  \"../input/lg-wwdd\"\n",
    "lg_ffgg1 = np.zeros(len(x_test))\n",
    "files = glob.glob(f'{MODEL_DIR}/*model*.pkl')\n",
    "for i, f in enumerate(files):\n",
    "    model = joblib.load(f)\n",
    "    lg_ffgg1 += model.predict(x_test)\n",
    "lg_ffgg1=lg_ffgg1/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aab2da61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:27:22.109857Z",
     "iopub.status.busy": "2021-09-27T11:27:22.109081Z",
     "iopub.status.idle": "2021-09-27T11:27:22.111072Z",
     "shell.execute_reply": "2021-09-27T11:27:22.111452Z"
    },
    "papermill": {
     "duration": 0.050988,
     "end_time": "2021-09-27T11:27:22.111587",
     "exception": false,
     "start_time": "2021-09-27T11:27:22.060599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import numpy.matlib\n",
    "\n",
    "target_name = 'target'\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e46ca724",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:27:22.241363Z",
     "iopub.status.busy": "2021-09-27T11:27:22.220593Z",
     "iopub.status.idle": "2021-09-27T11:27:23.148859Z",
     "shell.execute_reply": "2021-09-27T11:27:23.149448Z"
    },
    "papermill": {
     "duration": 0.995062,
     "end_time": "2021-09-27T11:27:23.149662",
     "exception": false,
     "start_time": "2021-09-27T11:27:22.154600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.7s finished\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../input/optiver-realized-volatility-prediction/'\n",
    "\n",
    "\n",
    "def calc_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "\n",
    "def log_return(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n",
    "\n",
    "\n",
    "def read_test():\n",
    "    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n",
    "  \n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    return test\n",
    "\n",
    "\n",
    "def book_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    \n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    \n",
    "    df['wap_mean'] = (df['wap1'] + df['wap2']) / 2\n",
    "\n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
    "    \n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "    \n",
    "    df['bas'] = (df[['ask_price1', 'ask_price2']].min(axis = 1)/ df[['bid_price1', 'bid_price2']].max(axis = 1) - 1) \n",
    "    df['h_spread_l1'] = df['ask_price1'] - df['bid_price1']\n",
    "    df['h_spread_l2'] = df['ask_price2'] - df['bid_price2']\n",
    "    df['log_return_bid_price1'] = np.log(df['bid_price1'].pct_change() + 1)\n",
    "    df['log_return_ask_price1'] = np.log(df['ask_price1'].pct_change() + 1)\n",
    "    df['log_return_bid_size1'] = np.log(df['bid_size1'].pct_change() + 1)\n",
    "    df['log_return_ask_size1'] = np.log(df['ask_size1'].pct_change() + 1)\n",
    "    df['log_ask_1_div_bid_1'] = np.log(df['ask_price1'] / df['bid_price1'])\n",
    "    df['log_ask_1_div_bid_1_size'] = np.log(df['ask_size1'] / df['bid_size1'])\n",
    "    df['log_return_bid_price2'] = np.log(df['bid_price2'].pct_change() + 1)\n",
    "    df['log_return_ask_price2'] = np.log(df['ask_price2'].pct_change() + 1)\n",
    "    df['log_return_bid_size2'] = np.log(df['bid_size2'].pct_change() + 1)\n",
    "    df['log_return_ask_size2'] = np.log(df['ask_size2'].pct_change() + 1)\n",
    "    df['log_ask_2_div_bid_2'] = np.log(df['ask_price2'] / df['bid_price2'])\n",
    "    df['log_ask_2_div_bid_2_size'] = np.log(df['ask_size2'] / df['bid_size2'])\n",
    "    \n",
    "   \n",
    "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
    "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
    "    \n",
    "   \n",
    "    create_feature_dict = {'wap1': [np.mean,np.max,np.std],\n",
    "        'wap2': [np.mean,np.max,np.std],\n",
    "\n",
    "        'log_return1': [realized_volatility,np.sum],\n",
    "        'log_return2': [realized_volatility,np.sum], \n",
    "        'log_return_bid_price1':[realized_volatility,np.sum], \n",
    "        'log_return_ask_price1':[realized_volatility,np.sum],\n",
    "        'log_return_bid_size1':[realized_volatility,np.sum], \n",
    "        'log_return_ask_size1':[realized_volatility,np.sum], \n",
    "        'log_ask_1_div_bid_1':[realized_volatility,np.sum], \n",
    "        'log_ask_1_div_bid_1_size':[realized_volatility,np.sum], \n",
    "        'log_return_bid_price2':[realized_volatility,np.sum], \n",
    "        'log_return_ask_price2':[realized_volatility,np.sum], \n",
    "        'log_return_bid_size2':[realized_volatility,np.sum], \n",
    "        'log_return_ask_size2':[realized_volatility,np.sum], \n",
    "        'log_ask_2_div_bid_2':[realized_volatility,np.sum], \n",
    "        'log_ask_2_div_bid_2_size':[realized_volatility,np.sum], \n",
    " \n",
    "        'wap_balance': [np.mean,np.max,np.std,np.sum], \n",
    "        'wap_mean': [np.mean,np.max,np.std,np.sum],\n",
    "        'price_spread':[np.mean,np.max,np.std,np.sum],\n",
    "        'bid_spread':[np.mean,np.max,np.std,np.sum],\n",
    "        'ask_spread':[np.mean,np.max,np.std,np.sum],\n",
    "        'total_volume':[np.mean,np.max,np.std,np.sum],\n",
    "        'volume_imbalance':[np.mean,np.max,np.std,np.sum],\n",
    "        'bas':[np.mean,np.max,np.std,np.sum],\n",
    "        'h_spread_l1':[np.mean,np.max,np.std,np.sum],\n",
    "        'h_spread_l2':[np.mean,np.max,np.std,np.sum],\n",
    "        'price_spread2':[np.mean,np.max,np.std,np.sum],\n",
    "        \"bid_ask_spread\":[np.mean,np.max,np.std,np.sum],}\n",
    "    \n",
    "    \n",
    "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
    "        \n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n",
    "       \n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "      \n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    " \n",
    "    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n",
    "\n",
    "   \n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "  \n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "\n",
    "def trade_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    \n",
    "    \n",
    "    create_feature_dict = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n",
    "        'order_count':[np.mean,np.sum,np.max],\n",
    "    }\n",
    "    \n",
    "  \n",
    "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
    "     \n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n",
    "       \n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "      \n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "\n",
    "   \n",
    "    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n",
    "    \n",
    "    def tendency(price, vol):    \n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff/price[1:])*100\n",
    "        power = np.sum(val*vol[1:])\n",
    "        return(power)\n",
    "    \n",
    "    lis = []\n",
    "    for n_time_id in df['time_id'].unique():\n",
    "        df_id = df[df['time_id'] == n_time_id]        \n",
    "        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n",
    "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
    "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
    "        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n",
    "        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n",
    "        \n",
    "        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n",
    "        energy = np.mean(df_id['price'].values**2)\n",
    "        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n",
    "        \n",
    "     \n",
    "        \n",
    "        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n",
    "        energy_v = np.sum(df_id['size'].values**2)\n",
    "        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n",
    "        \n",
    "        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n",
    "                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n",
    "    \n",
    "    df_lr = pd.DataFrame(lis)\n",
    "        \n",
    "   \n",
    "    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
    "    \n",
    "  \n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "   \n",
    "    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200','time_id'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "\n",
    "def get_time_stock(df):\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n",
    "   \n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    \n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "  \n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "   \n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "   \n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
    "    return df\n",
    "    \n",
    "\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    \n",
    "   \n",
    "    def for_joblib(stock_id):\n",
    "     \n",
    "        if is_train:\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "     \n",
    "        else:\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    "       \n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
    "        \n",
    "      \n",
    "        return df_tmp\n",
    "    \n",
    "  \n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "   \n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "test = read_test()\n",
    "\n",
    "\n",
    "test_stock_ids = test['stock_id'].unique()\n",
    "\n",
    "test_ = preprocessor(test_stock_ids, is_train = False)\n",
    "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
    "\n",
    "\n",
    "test = get_time_stock(test)\n",
    "\n",
    "test['size_tau'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique'] )\n",
    "test['size_tau_400'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_400'] )\n",
    "test['size_tau_300'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_300'] )\n",
    "test['size_tau_200'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_200'] )\n",
    "test['size_tau2'] = np.sqrt( 1/ test['trade_order_count_sum'] )\n",
    "test['size_tau2_400'] = np.sqrt( 0.25/ test['trade_order_count_sum'] )\n",
    "test['size_tau2_300'] = np.sqrt( 0.5/ test['trade_order_count_sum'] )\n",
    "test['size_tau2_200'] = np.sqrt( 0.75/ test['trade_order_count_sum'] )\n",
    "\n",
    "test['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0421c535",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:27:23.249926Z",
     "iopub.status.busy": "2021-09-27T11:27:23.248887Z",
     "iopub.status.idle": "2021-09-27T11:27:26.593257Z",
     "shell.execute_reply": "2021-09-27T11:27:26.592231Z"
    },
    "papermill": {
     "duration": 3.397847,
     "end_time": "2021-09-27T11:27:26.593463",
     "exception": false,
     "start_time": "2021-09-27T11:27:23.195616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(60)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(60)\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.backend import sigmoid\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow import keras\n",
    "def root_mean_squared_per_error(y_true, y_pred):\n",
    "         return K.sqrt(K.mean(K.square( (y_true - y_pred)/ y_true )))\n",
    "    \n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=20, verbose=0,\n",
    "    mode='min',restore_best_weights=True)\n",
    "\n",
    "plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.2, patience=7, verbose=0,\n",
    "    mode='min')\n",
    "colNames = list(test)\n",
    "colNames.remove('time_id')\n",
    "colNames.remove('row_id')\n",
    "colNames.remove('stock_id')\n",
    "test.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "from pickle import load,dump\n",
    "for col in colNames:\n",
    "    qt=load(open(f'/kaggle/input/nk-wwdd/qt_{col}.pkl', 'rb'))\n",
    "    test[col] = qt.transform(test[[col]])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c20ead8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:27:26.699381Z",
     "iopub.status.busy": "2021-09-27T11:27:26.698841Z",
     "iopub.status.idle": "2021-09-27T11:27:31.926507Z",
     "shell.execute_reply": "2021-09-27T11:27:31.925533Z"
    },
    "papermill": {
     "duration": 5.288702,
     "end_time": "2021-09-27T11:27:31.926663",
     "exception": false,
     "start_time": "2021-09-27T11:27:26.637961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 11, 22, 50, 55, 56, 62, 73, 76, 78, 84, 87, 96, 101, 112, 116, 122, 124, 126]\n",
      "[0, 4, 5, 10, 15, 16, 17, 23, 26, 28, 29, 36, 42, 44, 48, 53, 66, 69, 72, 85, 94, 95, 100, 102, 109, 111, 113, 115, 118, 120]\n",
      "[3, 6, 9, 18, 61, 63, 86, 97]\n",
      "[27, 31, 33, 37, 38, 40, 58, 59, 60, 74, 75, 77, 82, 83, 88, 89, 90, 98, 99, 110]\n",
      "[2, 7, 13, 14, 19, 20, 21, 30, 32, 34, 35, 39, 41, 43, 46, 47, 51, 52, 64, 67, 68, 70, 93, 103, 104, 105, 107, 108, 114, 119, 123, 125]\n",
      "[81]\n",
      "[8, 80]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "corr = train_p.corr()\n",
    "\n",
    "ids = corr.index\n",
    "\n",
    "label=[1, 0, 4, 2, 1, 1, 2, 4, 6, 2, 1, 0, 4, 4, 1, 1, 1, 2, 4, 4, 4, 0, 1, 1, 3, 1, 1, 4, 3, 4, 3, 4, 4, 1, 3, 3, 4,\n",
    " 3, 4, 1, 4, 1, 4, 4, 1, 0, 4, 4, 1, 0, 0, 3, 3, 3, 2, 0, 2, 4, 1, 4, 4, 1, 4, 1, 0, 3, 3, 0, 3, 0, 6, 5, 3, 3,\n",
    " 0, 1, 2, 0, 3, 3, 3, 4, 1, 1, 0, 2, 3, 3, 1, 0, 1, 4, 4, 4, 4, 4, 1, 3, 1, 0, 1, 4, 1, 0, 1, 4, 1, 0, 4, 0, 4,\n",
    " 0]\n",
    "\n",
    "l = []\n",
    "for n in range(7):\n",
    "    l.append ( [ (x-1) for x in ( (ids+1)*(label==n*np.ones(len(label)))) if x > 0] )\n",
    "    \n",
    "matTest = []\n",
    "\n",
    "n = 0\n",
    "for ind in l:\n",
    "    print(ind)\n",
    "    newDf = test.loc[test['stock_id'].isin(ind) ]    \n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    matTest.append ( newDf )\n",
    "    \n",
    "    n+=1  \n",
    "mat2 = pd.concat(matTest).reset_index()\n",
    "mat1=pd.read_csv('/kaggle/input/nk-wwdd/mat1.csv')\n",
    "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n",
    "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "mat2.reset_index(inplace=True)\n",
    "nnn = ['time_id',\n",
    "     'log_return1_realized_volatility_0c1',\n",
    "     'log_return1_realized_volatility_1c1',     \n",
    "     'log_return1_realized_volatility_3c1',\n",
    "     'log_return1_realized_volatility_4c1',     \n",
    "     'log_return1_realized_volatility_2c1',\n",
    "     'total_volume_mean_0c1',\n",
    "     'total_volume_mean_1c1', \n",
    "     'total_volume_mean_3c1',\n",
    "     'total_volume_mean_4c1', \n",
    "     'total_volume_mean_2c1',\n",
    "     'trade_size_mean_0c1',\n",
    "     'trade_size_mean_1c1', \n",
    "     'trade_size_mean_3c1',\n",
    "     'trade_size_mean_4c1', \n",
    "     'trade_size_mean_2c1',\n",
    "     'trade_order_count_mean_0c1',\n",
    "     'trade_order_count_mean_1c1',\n",
    "     'trade_order_count_mean_3c1',\n",
    "     'trade_order_count_mean_4c1',\n",
    "     'trade_order_count_mean_2c1',      \n",
    "     'price_spread_mean_0c1',\n",
    "     'price_spread_mean_1c1',\n",
    "     'price_spread_mean_3c1',\n",
    "     'price_spread_mean_4c1',\n",
    "     'price_spread_mean_2c1',   \n",
    "     'bid_spread_mean_0c1',\n",
    "     'bid_spread_mean_1c1',\n",
    "     'bid_spread_mean_3c1',\n",
    "     'bid_spread_mean_4c1',\n",
    "     'bid_spread_mean_2c1',       \n",
    "     'ask_spread_mean_0c1',\n",
    "     'ask_spread_mean_1c1',\n",
    "     'ask_spread_mean_3c1',\n",
    "     'ask_spread_mean_4c1',\n",
    "     'ask_spread_mean_2c1',   \n",
    "     'volume_imbalance_mean_0c1',\n",
    "     'volume_imbalance_mean_1c1',\n",
    "     'volume_imbalance_mean_3c1',\n",
    "     'volume_imbalance_mean_4c1',\n",
    "     'volume_imbalance_mean_2c1',       \n",
    "     'bid_ask_spread_mean_0c1',\n",
    "     'bid_ask_spread_mean_1c1',\n",
    "     'bid_ask_spread_mean_3c1',\n",
    "     'bid_ask_spread_mean_4c1',\n",
    "     'bid_ask_spread_mean_2c1',\n",
    "     'size_tau2_0c1',\n",
    "     'size_tau2_1c1',\n",
    "     'size_tau2_3c1',\n",
    "     'size_tau2_4c1',\n",
    "     'size_tau2_2c1'] \n",
    "test = pd.merge(test,mat2[nnn],how='left',on='time_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1558062b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:27:32.031714Z",
     "iopub.status.busy": "2021-09-27T11:27:32.030609Z",
     "iopub.status.idle": "2021-09-27T11:27:32.333454Z",
     "shell.execute_reply": "2021-09-27T11:27:32.332974Z"
    },
    "papermill": {
     "duration": 0.357186,
     "end_time": "2021-09-27T11:27:32.333587",
     "exception": false,
     "start_time": "2021-09-27T11:27:31.976401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def swish(x, beta = 1):\n",
    "    return (x * sigmoid(beta * x))\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "hidden_units = (128,64,32)\n",
    "stock_embedding_size = 24\n",
    "\n",
    "train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "cat_data =train['stock_id']\n",
    "del train\n",
    "gc.collect()\n",
    "\n",
    "def base_model():\n",
    "    \n",
    "    \n",
    "    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n",
    "    num_input = keras.Input(shape=(538), name='num_data')\n",
    "\n",
    "\n",
    "    \n",
    "    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n",
    "                                           input_length=1, name='stock_embedding')(stock_id_input)\n",
    "    stock_flattened = keras.layers.Flatten()(stock_embedded)\n",
    "    out = keras.layers.Concatenate()([stock_flattened, num_input])\n",
    "    \n",
    "    \n",
    "    for n_hidden in hidden_units:\n",
    "\n",
    "        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n",
    "        \n",
    "\n",
    "   \n",
    "    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
    "    \n",
    "    model = keras.Model(\n",
    "    inputs = [stock_id_input, num_input],\n",
    "    outputs = out,\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f631c532",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:27:32.430194Z",
     "iopub.status.busy": "2021-09-27T11:27:32.429680Z",
     "iopub.status.idle": "2021-09-27T11:27:32.514846Z",
     "shell.execute_reply": "2021-09-27T11:27:32.514436Z"
    },
    "papermill": {
     "duration": 0.136481,
     "end_time": "2021-09-27T11:27:32.514957",
     "exception": false,
     "start_time": "2021-09-27T11:27:32.378476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_to_consider = list(test)\n",
    "features_to_consider.remove('time_id')\n",
    "features_to_consider.remove('row_id')\n",
    "f_mean = np.load('/kaggle/input/nk-wwdd/f_mean.npy')\n",
    "for i in range(len(features_to_consider)):\n",
    "    test[features_to_consider[i]] = test[features_to_consider[i]].fillna(f_mean[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54dbf297",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:27:32.610578Z",
     "iopub.status.busy": "2021-09-27T11:27:32.609705Z",
     "iopub.status.idle": "2021-09-27T11:27:32.629747Z",
     "shell.execute_reply": "2021-09-27T11:27:32.630195Z"
    },
    "papermill": {
     "duration": 0.070588,
     "end_time": "2021-09-27T11:27:32.630320",
     "exception": false,
     "start_time": "2021-09-27T11:27:32.559732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>row_id</th>\n",
       "      <th>wap1_mean</th>\n",
       "      <th>wap1_amax</th>\n",
       "      <th>wap1_std</th>\n",
       "      <th>wap2_mean</th>\n",
       "      <th>wap2_amax</th>\n",
       "      <th>wap2_std</th>\n",
       "      <th>log_return1_realized_volatility</th>\n",
       "      <th>...</th>\n",
       "      <th>bid_ask_spread_mean_0c1</th>\n",
       "      <th>bid_ask_spread_mean_1c1</th>\n",
       "      <th>bid_ask_spread_mean_3c1</th>\n",
       "      <th>bid_ask_spread_mean_4c1</th>\n",
       "      <th>bid_ask_spread_mean_2c1</th>\n",
       "      <th>size_tau2_0c1</th>\n",
       "      <th>size_tau2_1c1</th>\n",
       "      <th>size_tau2_3c1</th>\n",
       "      <th>size_tau2_4c1</th>\n",
       "      <th>size_tau2_2c1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0-4</td>\n",
       "      <td>0.250325</td>\n",
       "      <td>-0.543764</td>\n",
       "      <td>-2.429043</td>\n",
       "      <td>0.335747</td>\n",
       "      <td>-0.527097</td>\n",
       "      <td>-2.723074</td>\n",
       "      <td>-3.421779</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.279017</td>\n",
       "      <td>0.823808</td>\n",
       "      <td>0.519245</td>\n",
       "      <td>-0.030445</td>\n",
       "      <td>0.358564</td>\n",
       "      <td>-0.086921</td>\n",
       "      <td>3.161571</td>\n",
       "      <td>0.590543</td>\n",
       "      <td>-0.161021</td>\n",
       "      <td>-0.106622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0-32</td>\n",
       "      <td>-0.001311</td>\n",
       "      <td>-0.000514</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>-0.001294</td>\n",
       "      <td>-0.000760</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.001459</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.279017</td>\n",
       "      <td>-0.260539</td>\n",
       "      <td>0.519245</td>\n",
       "      <td>-0.030445</td>\n",
       "      <td>0.358564</td>\n",
       "      <td>-0.086921</td>\n",
       "      <td>-0.085898</td>\n",
       "      <td>0.590543</td>\n",
       "      <td>-0.161021</td>\n",
       "      <td>-0.106622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0-34</td>\n",
       "      <td>-0.001311</td>\n",
       "      <td>-0.000514</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>-0.001294</td>\n",
       "      <td>-0.000760</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.001459</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.279017</td>\n",
       "      <td>-0.260539</td>\n",
       "      <td>0.519245</td>\n",
       "      <td>-0.030445</td>\n",
       "      <td>0.358564</td>\n",
       "      <td>-0.086921</td>\n",
       "      <td>-0.085898</td>\n",
       "      <td>0.590543</td>\n",
       "      <td>-0.161021</td>\n",
       "      <td>-0.106622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 541 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   stock_id  time_id row_id  wap1_mean  wap1_amax  wap1_std  wap2_mean  \\\n",
       "0         0        4    0-4   0.250325  -0.543764 -2.429043   0.335747   \n",
       "1         0       32   0-32  -0.001311  -0.000514  0.000281  -0.001294   \n",
       "2         0       34   0-34  -0.001311  -0.000514  0.000281  -0.001294   \n",
       "\n",
       "   wap2_amax  wap2_std  log_return1_realized_volatility  ...  \\\n",
       "0  -0.527097 -2.723074                        -3.421779  ...   \n",
       "1  -0.000760  0.000382                         0.001459  ...   \n",
       "2  -0.000760  0.000382                         0.001459  ...   \n",
       "\n",
       "   bid_ask_spread_mean_0c1  bid_ask_spread_mean_1c1  bid_ask_spread_mean_3c1  \\\n",
       "0                -0.279017                 0.823808                 0.519245   \n",
       "1                -0.279017                -0.260539                 0.519245   \n",
       "2                -0.279017                -0.260539                 0.519245   \n",
       "\n",
       "   bid_ask_spread_mean_4c1  bid_ask_spread_mean_2c1  size_tau2_0c1  \\\n",
       "0                -0.030445                 0.358564      -0.086921   \n",
       "1                -0.030445                 0.358564      -0.086921   \n",
       "2                -0.030445                 0.358564      -0.086921   \n",
       "\n",
       "   size_tau2_1c1  size_tau2_3c1  size_tau2_4c1  size_tau2_2c1  \n",
       "0       3.161571       0.590543      -0.161021      -0.106622  \n",
       "1      -0.085898       0.590543      -0.161021      -0.106622  \n",
       "2      -0.085898       0.590543      -0.161021      -0.106622  \n",
       "\n",
       "[3 rows x 541 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d9ff930",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:27:32.781023Z",
     "iopub.status.busy": "2021-09-27T11:27:32.780272Z",
     "iopub.status.idle": "2021-09-27T11:27:35.095699Z",
     "shell.execute_reply": "2021-09-27T11:27:35.096168Z"
    },
    "papermill": {
     "duration": 2.418881,
     "end_time": "2021-09-27T11:27:35.096321",
     "exception": false,
     "start_time": "2021-09-27T11:27:32.677440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_folds=5\n",
    "test_preds = np.zeros(len(test))\n",
    "\n",
    "for n_count in range(n_folds):\n",
    "    \n",
    "    model = base_model()\n",
    "    \n",
    "    model.compile(\n",
    "        keras.optimizers.Adam(learning_rate=0.004), \n",
    "        loss=root_mean_squared_per_error\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        features_to_consider.remove('stock_id')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    model.load_weights(f'../input/nk-wwdd/n1_model_{n_count}.hdf5')\n",
    "\n",
    "    scaler=load(open(f'/kaggle/input/nk-wwdd/scaler_{n_count}.pkl', 'rb'))\n",
    "    tt =scaler.transform(test[features_to_consider].values)\n",
    "    test_preds += model.predict([test['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n",
    "       \n",
    "    features_to_consider.append('stock_id')\n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "43e5c127",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:27:35.193400Z",
     "iopub.status.busy": "2021-09-27T11:27:35.192862Z",
     "iopub.status.idle": "2021-09-27T11:27:35.196630Z",
     "shell.execute_reply": "2021-09-27T11:27:35.196171Z"
    },
    "papermill": {
     "duration": 0.053962,
     "end_time": "2021-09-27T11:27:35.196732",
     "exception": false,
     "start_time": "2021-09-27T11:27:35.142770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nk_ffgg1=test_preds/n_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "777380da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:27:35.322381Z",
     "iopub.status.busy": "2021-09-27T11:27:35.321320Z",
     "iopub.status.idle": "2021-09-27T11:28:01.845047Z",
     "shell.execute_reply": "2021-09-27T11:28:01.844536Z"
    },
    "papermill": {
     "duration": 26.602375,
     "end_time": "2021-09-27T11:28:01.845189",
     "exception": false,
     "start_time": "2021-09-27T11:27:35.242814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ../input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc3d1d5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:28:01.948701Z",
     "iopub.status.busy": "2021-09-27T11:28:01.947911Z",
     "iopub.status.idle": "2021-09-27T11:28:01.953843Z",
     "shell.execute_reply": "2021-09-27T11:28:01.954196Z"
    },
    "papermill": {
     "duration": 0.061187,
     "end_time": "2021-09-27T11:28:01.954318",
     "exception": false,
     "start_time": "2021-09-27T11:28:01.893131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy.matlib\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "orange_black = [\n",
    "    '#fdc029', '#df861d', '#FF6347', '#aa3d01', '#a30e15', '#800000', '#171820'\n",
    "]\n",
    "plt.rcParams['figure.figsize'] = (16,9)\n",
    "plt.rcParams[\"figure.facecolor\"] = '#FFFACD'\n",
    "plt.rcParams[\"axes.facecolor\"] = '#FFFFE0'\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "plt.rcParams[\"grid.color\"] = orange_black[3]\n",
    "plt.rcParams[\"grid.alpha\"] = 0.5\n",
    "plt.rcParams[\"grid.linestyle\"] = '--'\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "63e51968",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:28:02.051812Z",
     "iopub.status.busy": "2021-09-27T11:28:02.050859Z",
     "iopub.status.idle": "2021-09-27T11:28:02.778208Z",
     "shell.execute_reply": "2021-09-27T11:28:02.778663Z"
    },
    "papermill": {
     "duration": 0.778411,
     "end_time": "2021-09-27T11:28:02.778823",
     "exception": false,
     "start_time": "2021-09-27T11:28:02.000412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../input/optiver-realized-volatility-prediction/'\n",
    "\n",
    "\n",
    "def calc_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap3(df):\n",
    "    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap4(df):\n",
    "    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "\n",
    "def log_return(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n",
    "\n",
    "\n",
    "def read_train_test():\n",
    "    \n",
    "    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n",
    "    \n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    \n",
    "    return test\n",
    "\n",
    "\n",
    "def book_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "   \n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "\n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    \n",
    "    df['wap_mean'] = (df['wap1'] + df['wap2']) / 2\n",
    "    \n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
    "\n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "    \n",
    "    df['bas'] = (df[['ask_price1', 'ask_price2']].min(axis = 1)/ df[['bid_price1', 'bid_price2']].max(axis = 1) - 1) \n",
    "    df['h_spread_l1'] = df['ask_price1'] - df['bid_price1']\n",
    "    df['h_spread_l2'] = df['ask_price2'] - df['bid_price2']\n",
    "    df['log_return_bid_price1'] = np.log(df['bid_price1'].pct_change() + 1)\n",
    "    df['log_return_ask_price1'] = np.log(df['ask_price1'].pct_change() + 1)\n",
    "    df['log_return_bid_size1'] = np.log(df['bid_size1'].pct_change() + 1)\n",
    "    df['log_return_ask_size1'] = np.log(df['ask_size1'].pct_change() + 1)\n",
    "    df['log_ask_1_div_bid_1'] = np.log(df['ask_price1'] / df['bid_price1'])\n",
    "    df['log_ask_1_div_bid_1_size'] = np.log(df['ask_size1'] / df['bid_size1'])\n",
    "    df['log_return_bid_price2'] = np.log(df['bid_price2'].pct_change() + 1)\n",
    "    df['log_return_ask_price2'] = np.log(df['ask_price2'].pct_change() + 1)\n",
    "    df['log_return_bid_size2'] = np.log(df['bid_size2'].pct_change() + 1)\n",
    "    df['log_return_ask_size2'] = np.log(df['ask_size2'].pct_change() + 1)\n",
    "    df['log_ask_2_div_bid_2'] = np.log(df['ask_price2'] / df['bid_price2'])\n",
    "    df['log_ask_2_div_bid_2_size'] = np.log(df['ask_size2'] / df['bid_size2'])\n",
    "    \n",
    "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
    "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    create_feature_dict = {\n",
    "        'wap1': [np.mean,np.max,np.std],\n",
    "        'wap2': [np.mean,np.max,np.std],\n",
    " \n",
    "        'log_return1': [realized_volatility,np.sum],\n",
    "        'log_return2': [realized_volatility,np.sum], \n",
    "        'log_return_bid_price1':[realized_volatility,np.sum],\n",
    "        'log_return_ask_price1':[realized_volatility,np.sum],\n",
    "        'log_return_bid_size1':[realized_volatility,np.sum],\n",
    "        'log_return_ask_size1':[realized_volatility,np.sum], \n",
    "        'log_ask_1_div_bid_1':[realized_volatility,np.sum],\n",
    "        'log_ask_1_div_bid_1_size':[realized_volatility,np.sum], \n",
    "        'log_return_bid_price2':[realized_volatility,np.sum], \n",
    "        'log_return_ask_price2':[realized_volatility,np.sum], \n",
    "        'log_return_bid_size2':[realized_volatility,np.sum],\n",
    "        'log_return_ask_size2':[realized_volatility,np.sum], \n",
    "        'log_ask_2_div_bid_2':[realized_volatility,np.sum], \n",
    "        'log_ask_2_div_bid_2_size':[realized_volatility,np.sum], \n",
    "      \n",
    "        'wap_balance': [np.mean,np.max,np.std,np.sum], \n",
    "        'wap_mean': [np.mean,np.max,np.std,np.sum],\n",
    "        'price_spread':[np.mean,np.max,np.std,np.sum],\n",
    "        'bid_spread':[np.mean,np.max,np.std,np.sum],\n",
    "        'ask_spread':[np.mean,np.max,np.std,np.sum],\n",
    "        'total_volume':[np.mean,np.max,np.std,np.sum],\n",
    "        'volume_imbalance':[np.mean,np.max,np.std,np.sum],\n",
    "        'bas':[np.mean,np.max,np.std,np.sum],\n",
    "        'h_spread_l1':[np.mean,np.max,np.std,np.sum],\n",
    "        'h_spread_l2':[np.mean,np.max,np.std,np.sum],\n",
    "        'price_spread2':[np.mean,np.max,np.std,np.sum],\n",
    "        \"bid_ask_spread\":[np.mean,np.max,np.std,np.sum],\n",
    "    }\n",
    "    \n",
    "    \n",
    "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
    "      \n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n",
    "       \n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "       \n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "\n",
    "    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n",
    "    \n",
    "\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "\n",
    "  \n",
    "    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "def trade_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    \n",
    "  \n",
    "    create_feature_dict = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n",
    "        'order_count':[np.mean,np.sum,np.max],\n",
    "    }\n",
    "    \n",
    "   \n",
    "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
    "       \n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n",
    "      \n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        \n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "\n",
    "  \n",
    "    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n",
    "    \n",
    "    def tendency(price, vol):    \n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff/price[1:])*100\n",
    "        power = np.sum(val*vol[1:])\n",
    "        return(power)\n",
    "    \n",
    "    lis = []\n",
    "    for n_time_id in df['time_id'].unique():\n",
    "        df_id = df[df['time_id'] == n_time_id]        \n",
    "        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n",
    "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
    "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
    "        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n",
    "        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n",
    "     \n",
    "        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n",
    "        energy = np.mean(df_id['price'].values**2)\n",
    "        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n",
    "        \n",
    "      \n",
    "        \n",
    "        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n",
    "        energy_v = np.sum(df_id['size'].values**2)\n",
    "        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n",
    "        \n",
    "        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n",
    "                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n",
    "    \n",
    "    df_lr = pd.DataFrame(lis)\n",
    "        \n",
    "   \n",
    "    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
    "    \n",
    "  \n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "\n",
    "    \n",
    "    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200','time_id'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "\n",
    "def get_time_stock(df):\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n",
    "\n",
    "\n",
    "  \n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "   \n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    \n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "   \n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    " \n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
    "    return df\n",
    "    \n",
    "\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    \n",
    "    \n",
    "    def for_joblib(stock_id):\n",
    "       \n",
    "        if is_train:\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "       \n",
    "        else:\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    " \n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
    "        \n",
    "       \n",
    "        return df_tmp\n",
    "    \n",
    "  \n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "   \n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "test = read_train_test()\n",
    "\n",
    " \n",
    "test_stock_ids = test['stock_id'].unique()\n",
    "\n",
    "test_ = preprocessor(test_stock_ids, is_train = False)\n",
    "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
    "\n",
    "\n",
    "test = get_time_stock(test)\n",
    "\n",
    "test['size_tau'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique'] )\n",
    "test['size_tau_400'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_400'] )\n",
    "test['size_tau_300'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_300'] )\n",
    "test['size_tau_200'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_200'] )\n",
    "test['size_tau2'] = np.sqrt( 1/ test['trade_order_count_sum'] )\n",
    "test['size_tau2_400'] = np.sqrt( 0.33/ test['trade_order_count_sum'] )\n",
    "test['size_tau2_300'] = np.sqrt( 0.5/ test['trade_order_count_sum'] )\n",
    "test['size_tau2_200'] = np.sqrt( 0.66/ test['trade_order_count_sum'] )\n",
    "\n",
    "\n",
    "test['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "565cfdd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:28:02.889873Z",
     "iopub.status.busy": "2021-09-27T11:28:02.889237Z",
     "iopub.status.idle": "2021-09-27T11:28:08.440762Z",
     "shell.execute_reply": "2021-09-27T11:28:08.440277Z"
    },
    "papermill": {
     "duration": 5.614526,
     "end_time": "2021-09-27T11:28:08.440896",
     "exception": false,
     "start_time": "2021-09-27T11:28:02.826370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 11, 22, 50, 55, 56, 62, 73, 76, 78, 84, 87, 96, 101, 112, 116, 122, 124, 126]\n",
      "[0, 4, 5, 10, 15, 16, 17, 23, 26, 28, 29, 36, 42, 44, 48, 53, 66, 69, 72, 85, 94, 95, 100, 102, 109, 111, 113, 115, 118, 120]\n",
      "[3, 6, 9, 18, 61, 63, 86, 97]\n",
      "[27, 31, 33, 37, 38, 40, 58, 59, 60, 74, 75, 77, 82, 83, 88, 89, 90, 98, 99, 110]\n",
      "[2, 7, 13, 14, 19, 20, 21, 30, 32, 34, 35, 39, 41, 43, 46, 47, 51, 52, 64, 67, 68, 70, 93, 103, 104, 105, 107, 108, 114, 119, 123, 125]\n",
      "[81]\n",
      "[8, 80]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "corr = train_p.corr()\n",
    "\n",
    "ids = corr.index\n",
    "\n",
    "label=[1, 0, 4, 2, 1, 1, 2, 4, 6, 2, 1, 0, 4, 4, 1, 1, 1, 2, 4, 4, 4, 0, 1, 1, 3, 1, 1, 4, 3, 4, 3, 4, 4, 1, 3, 3, 4,\n",
    " 3, 4, 1, 4, 1, 4, 4, 1, 0, 4, 4, 1, 0, 0, 3, 3, 3, 2, 0, 2, 4, 1, 4, 4, 1, 4, 1, 0, 3, 3, 0, 3, 0, 6, 5, 3, 3,\n",
    " 0, 1, 2, 0, 3, 3, 3, 4, 1, 1, 0, 2, 3, 3, 1, 0, 1, 4, 4, 4, 4, 4, 1, 3, 1, 0, 1, 4, 1, 0, 1, 4, 1, 0, 4, 0, 4,\n",
    " 0]\n",
    "\n",
    "l = []\n",
    "for n in range(7):\n",
    "    l.append ( [ (x-1) for x in ( (ids+1)*(label==n*np.ones(len(label)))) if x > 0] )\n",
    "    \n",
    "\n",
    "\n",
    "matTest = []\n",
    "\n",
    "n = 0\n",
    "for ind in l:\n",
    "    print(ind)\n",
    "    \n",
    "    newDf = test.loc[test['stock_id'].isin(ind) ]    \n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    matTest.append ( newDf )\n",
    "    \n",
    "    n+=1\n",
    "\n",
    "mat2 = pd.concat(matTest).reset_index()\n",
    "mat1=pd.read_csv('/kaggle/input/tk-wwdd/mat1.csv')\n",
    "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n",
    "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "mat2.reset_index(inplace=True)\n",
    "nnn = ['time_id',\n",
    "     'log_return1_realized_volatility_0c1',\n",
    "     'log_return1_realized_volatility_1c1',     \n",
    "     'log_return1_realized_volatility_3c1',\n",
    "     'log_return1_realized_volatility_4c1',     \n",
    "     'log_return1_realized_volatility_2c1',\n",
    "     'total_volume_mean_0c1',\n",
    "     'total_volume_mean_1c1', \n",
    "     'total_volume_mean_3c1',\n",
    "     'total_volume_mean_4c1', \n",
    "     'total_volume_mean_2c1',\n",
    "     'trade_size_mean_0c1',\n",
    "     'trade_size_mean_1c1', \n",
    "     'trade_size_mean_3c1',\n",
    "     'trade_size_mean_4c1', \n",
    "     'trade_size_mean_2c1',\n",
    "     'trade_order_count_mean_0c1',\n",
    "     'trade_order_count_mean_1c1',\n",
    "     'trade_order_count_mean_3c1',\n",
    "     'trade_order_count_mean_4c1',\n",
    "     'trade_order_count_mean_2c1',      \n",
    "     'price_spread_mean_0c1',\n",
    "     'price_spread_mean_1c1',\n",
    "     'price_spread_mean_3c1',\n",
    "     'price_spread_mean_4c1',\n",
    "     'price_spread_mean_2c1',   \n",
    "     'bid_spread_mean_0c1',\n",
    "     'bid_spread_mean_1c1',\n",
    "     'bid_spread_mean_3c1',\n",
    "     'bid_spread_mean_4c1',\n",
    "     'bid_spread_mean_2c1',       \n",
    "     'ask_spread_mean_0c1',\n",
    "     'ask_spread_mean_1c1',\n",
    "     'ask_spread_mean_3c1',\n",
    "     'ask_spread_mean_4c1',\n",
    "     'ask_spread_mean_2c1',   \n",
    "     'volume_imbalance_mean_0c1',\n",
    "     'volume_imbalance_mean_1c1',\n",
    "     'volume_imbalance_mean_3c1',\n",
    "     'volume_imbalance_mean_4c1',\n",
    "     'volume_imbalance_mean_2c1',       \n",
    "     'bid_ask_spread_mean_0c1',\n",
    "     'bid_ask_spread_mean_1c1',\n",
    "     'bid_ask_spread_mean_3c1',\n",
    "     'bid_ask_spread_mean_4c1',\n",
    "     'bid_ask_spread_mean_2c1',\n",
    "     'size_tau2_0c1',\n",
    "     'size_tau2_1c1',\n",
    "     'size_tau2_3c1',\n",
    "     'size_tau2_4c1',\n",
    "     'size_tau2_2c1'] \n",
    "test = pd.merge(test,mat2[nnn],how='left',on='time_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "74cc0b0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:28:08.541106Z",
     "iopub.status.busy": "2021-09-27T11:28:08.540616Z",
     "iopub.status.idle": "2021-09-27T11:28:08.625683Z",
     "shell.execute_reply": "2021-09-27T11:28:08.626092Z"
    },
    "papermill": {
     "duration": 0.137386,
     "end_time": "2021-09-27T11:28:08.626218",
     "exception": false,
     "start_time": "2021-09-27T11:28:08.488832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = [col for col in test.columns.tolist() if col not in ['time_id','row_id']]\n",
    "f_mean = np.load('/kaggle/input/tk-wwdd/f_mean.npy')\n",
    "for i in range(len(features)):\n",
    "    test[features[i]] = test[features[i]].fillna(f_mean[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9e058eb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:28:08.730064Z",
     "iopub.status.busy": "2021-09-27T11:28:08.728803Z",
     "iopub.status.idle": "2021-09-27T11:28:10.335647Z",
     "shell.execute_reply": "2021-09-27T11:28:10.336098Z"
    },
    "papermill": {
     "duration": 1.662595,
     "end_time": "2021-09-27T11:28:10.336244",
     "exception": false,
     "start_time": "2021-09-27T11:28:08.673649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test=test.copy()\n",
    "X_test.drop(['time_id','row_id'], axis=1,inplace=True)\n",
    "def rmspe(y_true, y_pred):\n",
    "    \n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "class RMSPE(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"rmspe\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        \n",
    "        return np.sqrt(np.mean(np.square((y_true - y_score) / y_true)))\n",
    "    \n",
    "\n",
    "\n",
    "def RMSPELoss(y_pred, y_true):\n",
    "    return torch.sqrt(torch.mean( ((y_true - y_pred) / y_true) ** 2 )).clone()\n",
    "from pickle import load,dump\n",
    "\n",
    "for col in features:\n",
    "    if  col == 'stock_id':\n",
    "        l_enc=load(open('/kaggle/input/tk-wwdd/l_enc.pkl', 'rb'))\n",
    "        X_test[col] = l_enc.transform(X_test[col].values)\n",
    "    else:\n",
    "        scaler=load(open(f'/kaggle/input/tk-wwdd/scaler_{col}.pkl', 'rb'))\n",
    "        X_test[col] = scaler.transform(X_test[col].values.reshape(-1, 1))\n",
    "cat_idxs_df=pd.read_csv('/kaggle/input/tk-wwdd/cat_idxs_df.csv')\n",
    "cat_dims_df=pd.read_csv('/kaggle/input/tk-wwdd/cat_dims_df.csv')\n",
    "cat_idxs=cat_idxs_df['cat_idxs']\n",
    "cat_dims=cat_dims_df['cat_dims']\n",
    "tabnet_params = dict(\n",
    "    cat_idxs=cat_idxs,\n",
    "    cat_dims=cat_dims,\n",
    "    cat_emb_dim=1,\n",
    "    n_d = 13,\n",
    "    n_a = 13,\n",
    "    n_steps = 1,\n",
    "    gamma = 2,\n",
    "    n_independent = 2,\n",
    "    n_shared = 2,\n",
    "    lambda_sparse = 0,\n",
    "    optimizer_fn = Adam,\n",
    "    optimizer_params = dict(lr = (2e-2)),\n",
    "    mask_type = \"entmax\",\n",
    "    scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False),\n",
    "    scheduler_fn = CosineAnnealingWarmRestarts,\n",
    "    seed = 1,\n",
    "    verbose = 10\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "738aea73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:28:10.459767Z",
     "iopub.status.busy": "2021-09-27T11:28:10.454977Z",
     "iopub.status.idle": "2021-09-27T11:28:10.462309Z",
     "shell.execute_reply": "2021-09-27T11:28:10.462723Z"
    },
    "papermill": {
     "duration": 0.078368,
     "end_time": "2021-09-27T11:28:10.462849",
     "exception": false,
     "start_time": "2021-09-27T11:28:10.384481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>wap1_mean</th>\n",
       "      <th>wap1_amax</th>\n",
       "      <th>wap1_std</th>\n",
       "      <th>wap2_mean</th>\n",
       "      <th>wap2_amax</th>\n",
       "      <th>wap2_std</th>\n",
       "      <th>log_return1_realized_volatility</th>\n",
       "      <th>log_return1_sum</th>\n",
       "      <th>log_return2_realized_volatility</th>\n",
       "      <th>...</th>\n",
       "      <th>bid_ask_spread_mean_0c1</th>\n",
       "      <th>bid_ask_spread_mean_1c1</th>\n",
       "      <th>bid_ask_spread_mean_3c1</th>\n",
       "      <th>bid_ask_spread_mean_4c1</th>\n",
       "      <th>bid_ask_spread_mean_2c1</th>\n",
       "      <th>size_tau2_0c1</th>\n",
       "      <th>size_tau2_1c1</th>\n",
       "      <th>size_tau2_3c1</th>\n",
       "      <th>size_tau2_4c1</th>\n",
       "      <th>size_tau2_2c1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.191565e-01</td>\n",
       "      <td>-4.375152e-01</td>\n",
       "      <td>-8.941496e-01</td>\n",
       "      <td>1.623597e-01</td>\n",
       "      <td>-4.365765e-01</td>\n",
       "      <td>-9.351345e-01</td>\n",
       "      <td>-1.098426e+00</td>\n",
       "      <td>8.241560e-02</td>\n",
       "      <td>-1.105520e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.248494e-13</td>\n",
       "      <td>1.434182e+00</td>\n",
       "      <td>4.230394e-13</td>\n",
       "      <td>2.611967e-13</td>\n",
       "      <td>-1.809262e-13</td>\n",
       "      <td>-2.844108e-13</td>\n",
       "      <td>2.003125e+01</td>\n",
       "      <td>-5.454171e-13</td>\n",
       "      <td>-1.019213e-12</td>\n",
       "      <td>-1.264623e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-2.042257e-12</td>\n",
       "      <td>1.225712e-12</td>\n",
       "      <td>3.505232e-15</td>\n",
       "      <td>9.872247e-13</td>\n",
       "      <td>-5.769340e-14</td>\n",
       "      <td>9.161438e-15</td>\n",
       "      <td>9.433118e-15</td>\n",
       "      <td>4.194278e-17</td>\n",
       "      <td>9.664863e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.248494e-13</td>\n",
       "      <td>2.431447e-13</td>\n",
       "      <td>4.230394e-13</td>\n",
       "      <td>2.611967e-13</td>\n",
       "      <td>-1.809262e-13</td>\n",
       "      <td>-2.844108e-13</td>\n",
       "      <td>6.856557e-13</td>\n",
       "      <td>-5.454171e-13</td>\n",
       "      <td>-1.019213e-12</td>\n",
       "      <td>-1.264623e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-2.042257e-12</td>\n",
       "      <td>1.225712e-12</td>\n",
       "      <td>3.505232e-15</td>\n",
       "      <td>9.872247e-13</td>\n",
       "      <td>-5.769340e-14</td>\n",
       "      <td>9.161438e-15</td>\n",
       "      <td>9.433118e-15</td>\n",
       "      <td>4.194278e-17</td>\n",
       "      <td>9.664863e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.248494e-13</td>\n",
       "      <td>2.431447e-13</td>\n",
       "      <td>4.230394e-13</td>\n",
       "      <td>2.611967e-13</td>\n",
       "      <td>-1.809262e-13</td>\n",
       "      <td>-2.844108e-13</td>\n",
       "      <td>6.856557e-13</td>\n",
       "      <td>-5.454171e-13</td>\n",
       "      <td>-1.019213e-12</td>\n",
       "      <td>-1.264623e-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 539 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   stock_id     wap1_mean     wap1_amax      wap1_std     wap2_mean  \\\n",
       "0         0  1.191565e-01 -4.375152e-01 -8.941496e-01  1.623597e-01   \n",
       "1         0 -2.042257e-12  1.225712e-12  3.505232e-15  9.872247e-13   \n",
       "2         0 -2.042257e-12  1.225712e-12  3.505232e-15  9.872247e-13   \n",
       "\n",
       "      wap2_amax      wap2_std  log_return1_realized_volatility  \\\n",
       "0 -4.365765e-01 -9.351345e-01                    -1.098426e+00   \n",
       "1 -5.769340e-14  9.161438e-15                     9.433118e-15   \n",
       "2 -5.769340e-14  9.161438e-15                     9.433118e-15   \n",
       "\n",
       "   log_return1_sum  log_return2_realized_volatility  ...  \\\n",
       "0     8.241560e-02                    -1.105520e+00  ...   \n",
       "1     4.194278e-17                     9.664863e-15  ...   \n",
       "2     4.194278e-17                     9.664863e-15  ...   \n",
       "\n",
       "   bid_ask_spread_mean_0c1  bid_ask_spread_mean_1c1  bid_ask_spread_mean_3c1  \\\n",
       "0             1.248494e-13             1.434182e+00             4.230394e-13   \n",
       "1             1.248494e-13             2.431447e-13             4.230394e-13   \n",
       "2             1.248494e-13             2.431447e-13             4.230394e-13   \n",
       "\n",
       "   bid_ask_spread_mean_4c1  bid_ask_spread_mean_2c1  size_tau2_0c1  \\\n",
       "0             2.611967e-13            -1.809262e-13  -2.844108e-13   \n",
       "1             2.611967e-13            -1.809262e-13  -2.844108e-13   \n",
       "2             2.611967e-13            -1.809262e-13  -2.844108e-13   \n",
       "\n",
       "   size_tau2_1c1  size_tau2_3c1  size_tau2_4c1  size_tau2_2c1  \n",
       "0   2.003125e+01  -5.454171e-13  -1.019213e-12  -1.264623e-13  \n",
       "1   6.856557e-13  -5.454171e-13  -1.019213e-12  -1.264623e-13  \n",
       "2   6.856557e-13  -5.454171e-13  -1.019213e-12  -1.264623e-13  \n",
       "\n",
       "[3 rows x 539 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "62e8a1ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:28:10.564839Z",
     "iopub.status.busy": "2021-09-27T11:28:10.564281Z",
     "iopub.status.idle": "2021-09-27T11:28:10.600435Z",
     "shell.execute_reply": "2021-09-27T11:28:10.599971Z"
    },
    "papermill": {
     "duration": 0.087989,
     "end_time": "2021-09-27T11:28:10.600543",
     "exception": false,
     "start_time": "2021-09-27T11:28:10.512554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "pathB = \"../input/tk-wwdd/\"\n",
    "modelpath = [os.path.join(pathB,s) for s in os.listdir(pathB) if (\"zip\" in s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8dd876cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:28:10.702522Z",
     "iopub.status.busy": "2021-09-27T11:28:10.701946Z",
     "iopub.status.idle": "2021-09-27T11:28:11.131716Z",
     "shell.execute_reply": "2021-09-27T11:28:11.132245Z"
    },
    "papermill": {
     "duration": 0.483076,
     "end_time": "2021-09-27T11:28:11.132453",
     "exception": false,
     "start_time": "2021-09-27T11:28:10.649377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n"
     ]
    }
   ],
   "source": [
    "test_predictions=[]\n",
    "for path in modelpath:\n",
    "    clf =  TabNetRegressor(**tabnet_params)\n",
    "    clf.load_model(path)\n",
    "    test_predictions.append(clf.predict(X_test.values).squeeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5fea6876",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:28:11.235950Z",
     "iopub.status.busy": "2021-09-27T11:28:11.234481Z",
     "iopub.status.idle": "2021-09-27T11:28:11.236632Z",
     "shell.execute_reply": "2021-09-27T11:28:11.237026Z"
    },
    "papermill": {
     "duration": 0.054798,
     "end_time": "2021-09-27T11:28:11.237152",
     "exception": false,
     "start_time": "2021-09-27T11:28:11.182354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tk_ffgg1 =  np.mean(test_predictions,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "41327a3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:28:11.341363Z",
     "iopub.status.busy": "2021-09-27T11:28:11.339976Z",
     "iopub.status.idle": "2021-09-27T11:28:11.342114Z",
     "shell.execute_reply": "2021-09-27T11:28:11.342518Z"
    },
    "papermill": {
     "duration": 0.05687,
     "end_time": "2021-09-27T11:28:11.342645",
     "exception": false,
     "start_time": "2021-09-27T11:28:11.285775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import numpy.matlib\n",
    "\n",
    "target_name = 'target'\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e6a562bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:28:11.478785Z",
     "iopub.status.busy": "2021-09-27T11:28:11.442467Z",
     "iopub.status.idle": "2021-09-27T11:28:11.858266Z",
     "shell.execute_reply": "2021-09-27T11:28:11.858875Z"
    },
    "papermill": {
     "duration": 0.468128,
     "end_time": "2021-09-27T11:28:11.859067",
     "exception": false,
     "start_time": "2021-09-27T11:28:11.390939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../input/optiver-realized-volatility-prediction/'\n",
    "\n",
    "def calc_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap3(df):\n",
    "    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap4(df):\n",
    "    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "\n",
    "def log_return(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n",
    "\n",
    "\n",
    "def read_test():\n",
    "    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n",
    "   \n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    return test\n",
    "\n",
    "\n",
    "def book_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "\n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    df['wap3'] = calc_wap3(df)\n",
    "    df['wap4'] = calc_wap4(df)\n",
    " \n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n",
    "    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n",
    "\n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
    "   \n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "    \n",
    "   \n",
    "    create_feature_dict = {\n",
    "        'wap1': [np.sum, np.std],\n",
    "        'wap2': [np.sum, np.std],\n",
    "        'wap3': [np.sum, np.std],\n",
    "        'wap4': [np.sum, np.std],\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "        'wap_balance': [np.sum, np.max],\n",
    "        'price_spread':[np.sum, np.max],\n",
    "        'price_spread2':[np.sum, np.max],\n",
    "        'bid_spread':[np.sum, np.max],\n",
    "        'ask_spread':[np.sum, np.max],\n",
    "        'total_volume':[np.sum, np.max],\n",
    "        'volume_imbalance':[np.sum, np.max],\n",
    "        \"bid_ask_spread\":[np.sum,  np.max],\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "    }\n",
    "    \n",
    " \n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "    \n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "      \n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "  \n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "   \n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "\n",
    " \n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "   \n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    " \n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "\n",
    "def trade_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    df['amount']=df['price']*df['size']\n",
    "   \n",
    "    create_feature_dict = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum, np.max, np.min],\n",
    "        'order_count':[np.sum,np.max],\n",
    "        'amount':[np.sum,np.max,np.min],\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum],\n",
    "        'order_count':[np.sum],\n",
    "    }\n",
    " \n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "       \n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "       \n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "   \n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "\n",
    "\n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "    \n",
    "    def tendency(price, vol):    \n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff/price[1:])*100\n",
    "        power = np.sum(val*vol[1:])\n",
    "        return(power)\n",
    "    \n",
    "    lis = []\n",
    "    for n_time_id in df['time_id'].unique():\n",
    "        df_id = df[df['time_id'] == n_time_id]        \n",
    "        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n",
    "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
    "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
    "        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n",
    "        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n",
    "  \n",
    "        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n",
    "        energy = np.mean(df_id['price'].values**2)\n",
    "        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n",
    "        \n",
    "\n",
    "        \n",
    "        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n",
    "        energy_v = np.sum(df_id['size'].values**2)\n",
    "        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n",
    "        \n",
    "        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n",
    "                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n",
    "    \n",
    "    df_lr = pd.DataFrame(lis)\n",
    "        \n",
    "   \n",
    "    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
    "    \n",
    "  \n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "   \n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "\n",
    "def get_time_stock(df):\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n",
    "\n",
    "\n",
    "  \n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "  \n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    " \n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "  \n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "\n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
    "    return df\n",
    "    \n",
    "\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    \n",
    "  \n",
    "    def for_joblib(stock_id):\n",
    " \n",
    "        if is_train:\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    "        else:\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    "     \n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
    "        \n",
    "       \n",
    "        return df_tmp\n",
    "    \n",
    "  \n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "  \n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "test = read_test()\n",
    "\n",
    "\n",
    "test_stock_ids = test['stock_id'].unique()\n",
    "\n",
    "test_ = preprocessor(test_stock_ids, is_train = False)\n",
    "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
    "\n",
    "\n",
    "test = get_time_stock(test)\n",
    "\n",
    "test['size_tau'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique'] )\n",
    "test['size_tau_400'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_400'] )\n",
    "test['size_tau_300'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_300'] )\n",
    "test['size_tau_200'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_200'] )\n",
    "test['size_tau2'] = np.sqrt( 1/ test['trade_order_count_sum'] )\n",
    "test['size_tau2_400'] = np.sqrt( 0.33/ test['trade_order_count_sum'] )\n",
    "test['size_tau2_300'] = np.sqrt( 0.5/ test['trade_order_count_sum'] )\n",
    "test['size_tau2_200'] = np.sqrt( 0.66/ test['trade_order_count_sum'] )\n",
    "\n",
    "\n",
    "test['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "094ec047",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:28:11.973605Z",
     "iopub.status.busy": "2021-09-27T11:28:11.969131Z",
     "iopub.status.idle": "2021-09-27T11:28:14.287926Z",
     "shell.execute_reply": "2021-09-27T11:28:14.286932Z"
    },
    "papermill": {
     "duration": 2.37763,
     "end_time": "2021-09-27T11:28:14.288082",
     "exception": false,
     "start_time": "2021-09-27T11:28:11.910452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 11, 22, 50, 55, 56, 62, 73, 76, 78, 84, 87, 96, 101, 112, 116, 122, 124, 126]\n",
      "[0, 4, 5, 10, 15, 16, 17, 23, 26, 28, 29, 36, 42, 44, 48, 53, 66, 69, 72, 85, 94, 95, 100, 102, 109, 111, 113, 115, 118, 120]\n",
      "[3, 6, 9, 18, 61, 63, 86, 97]\n",
      "[27, 31, 33, 37, 38, 40, 58, 59, 60, 74, 75, 77, 82, 83, 88, 89, 90, 98, 99, 110]\n",
      "[2, 7, 13, 14, 19, 20, 21, 30, 32, 34, 35, 39, 41, 43, 46, 47, 51, 52, 64, 67, 68, 70, 93, 103, 104, 105, 107, 108, 114, 119, 123, 125]\n",
      "[81]\n",
      "[8, 80]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "corr = train_p.corr()\n",
    "\n",
    "ids = corr.index\n",
    "\n",
    "label=[1, 0, 4, 2, 1, 1, 2, 4, 6, 2, 1, 0, 4, 4, 1, 1, 1, 2, 4, 4, 4, 0, 1, 1, 3, 1, 1, 4, 3, 4, 3, 4, 4, 1, 3, 3, 4,\n",
    " 3, 4, 1, 4, 1, 4, 4, 1, 0, 4, 4, 1, 0, 0, 3, 3, 3, 2, 0, 2, 4, 1, 4, 4, 1, 4, 1, 0, 3, 3, 0, 3, 0, 6, 5, 3, 3,\n",
    " 0, 1, 2, 0, 3, 3, 3, 4, 1, 1, 0, 2, 3, 3, 1, 0, 1, 4, 4, 4, 4, 4, 1, 3, 1, 0, 1, 4, 1, 0, 1, 4, 1, 0, 4, 0, 4,\n",
    " 0]\n",
    "\n",
    "l = []\n",
    "for n in range(7):\n",
    "    l.append ( [ (x-1) for x in ( (ids+1)*(label==n*np.ones(len(label)))) if x > 0] )\n",
    "    \n",
    "matTest = []\n",
    "\n",
    "n = 0\n",
    "for ind in l:\n",
    "    print(ind)\n",
    "    newDf = test.loc[test['stock_id'].isin(ind) ]    \n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    matTest.append ( newDf )\n",
    "    \n",
    "    n+=1  \n",
    "mat2 = pd.concat(matTest).reset_index()\n",
    "mat1=pd.read_csv('/kaggle/input/lk-wwdd/mat1.csv')\n",
    "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n",
    "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "mat2.reset_index(inplace=True)\n",
    "nnn = ['time_id',\n",
    "     'log_return1_realized_volatility_0c1',\n",
    "     'log_return1_realized_volatility_1c1',     \n",
    "     'log_return1_realized_volatility_3c1',\n",
    "     'log_return1_realized_volatility_4c1',     \n",
    "     'log_return1_realized_volatility_2c1',\n",
    "     'total_volume_sum_0c1',\n",
    "     'total_volume_sum_1c1', \n",
    "     'total_volume_sum_3c1',\n",
    "     'total_volume_sum_4c1', \n",
    "     'total_volume_sum_2c1',\n",
    "     'trade_size_sum_0c1',\n",
    "     'trade_size_sum_1c1', \n",
    "     'trade_size_sum_3c1',\n",
    "     'trade_size_sum_4c1', \n",
    "     'trade_size_sum_2c1',\n",
    "     'trade_order_count_sum_0c1',\n",
    "     'trade_order_count_sum_1c1',\n",
    "     'trade_order_count_sum_3c1',\n",
    "     'trade_order_count_sum_4c1',\n",
    "     'trade_order_count_sum_2c1',      \n",
    "     'price_spread_sum_0c1',\n",
    "     'price_spread_sum_1c1',\n",
    "     'price_spread_sum_3c1',\n",
    "     'price_spread_sum_4c1',\n",
    "     'price_spread_sum_2c1',   \n",
    "     'bid_spread_sum_0c1',\n",
    "     'bid_spread_sum_1c1',\n",
    "     'bid_spread_sum_3c1',\n",
    "     'bid_spread_sum_4c1',\n",
    "     'bid_spread_sum_2c1',       \n",
    "     'ask_spread_sum_0c1',\n",
    "     'ask_spread_sum_1c1',\n",
    "     'ask_spread_sum_3c1',\n",
    "     'ask_spread_sum_4c1',\n",
    "     'ask_spread_sum_2c1',   \n",
    "     'volume_imbalance_sum_0c1',\n",
    "     'volume_imbalance_sum_1c1',\n",
    "     'volume_imbalance_sum_3c1',\n",
    "     'volume_imbalance_sum_4c1',\n",
    "     'volume_imbalance_sum_2c1',       \n",
    "     'bid_ask_spread_sum_0c1',\n",
    "     'bid_ask_spread_sum_1c1',\n",
    "     'bid_ask_spread_sum_3c1',\n",
    "     'bid_ask_spread_sum_4c1',\n",
    "     'bid_ask_spread_sum_2c1',\n",
    "     'size_tau2_0c1',\n",
    "     'size_tau2_1c1',\n",
    "     'size_tau2_3c1',\n",
    "     'size_tau2_4c1',\n",
    "     'size_tau2_2c1'] \n",
    "test = pd.merge(test,mat2[nnn],how='left',on='time_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c43264d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:28:14.415054Z",
     "iopub.status.busy": "2021-09-27T11:28:14.404257Z",
     "iopub.status.idle": "2021-09-27T11:28:14.417606Z",
     "shell.execute_reply": "2021-09-27T11:28:14.418062Z"
    },
    "papermill": {
     "duration": 0.076017,
     "end_time": "2021-09-27T11:28:14.418196",
     "exception": false,
     "start_time": "2021-09-27T11:28:14.342179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>row_id</th>\n",
       "      <th>wap1_sum</th>\n",
       "      <th>wap1_std</th>\n",
       "      <th>wap2_sum</th>\n",
       "      <th>wap2_std</th>\n",
       "      <th>wap3_sum</th>\n",
       "      <th>wap3_std</th>\n",
       "      <th>wap4_sum</th>\n",
       "      <th>...</th>\n",
       "      <th>bid_ask_spread_sum_0c1</th>\n",
       "      <th>bid_ask_spread_sum_1c1</th>\n",
       "      <th>bid_ask_spread_sum_3c1</th>\n",
       "      <th>bid_ask_spread_sum_4c1</th>\n",
       "      <th>bid_ask_spread_sum_2c1</th>\n",
       "      <th>size_tau2_0c1</th>\n",
       "      <th>size_tau2_1c1</th>\n",
       "      <th>size_tau2_3c1</th>\n",
       "      <th>size_tau2_4c1</th>\n",
       "      <th>size_tau2_2c1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0-4</td>\n",
       "      <td>3.001215</td>\n",
       "      <td>0.00017</td>\n",
       "      <td>3.00165</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>3.000752</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>2.999481</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001524</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.301511</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0-32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0-34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 247 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   stock_id  time_id row_id  wap1_sum  wap1_std  wap2_sum  wap2_std  wap3_sum  \\\n",
       "0         0        4    0-4  3.001215   0.00017   3.00165  0.000153  3.000752   \n",
       "1         0       32   0-32       NaN       NaN       NaN       NaN       NaN   \n",
       "2         0       34   0-34       NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "   wap3_std  wap4_sum  ...  bid_ask_spread_sum_0c1  bid_ask_spread_sum_1c1  \\\n",
       "0  0.000142  2.999481  ...                     NaN                0.001524   \n",
       "1       NaN       NaN  ...                     NaN                     NaN   \n",
       "2       NaN       NaN  ...                     NaN                     NaN   \n",
       "\n",
       "   bid_ask_spread_sum_3c1  bid_ask_spread_sum_4c1  bid_ask_spread_sum_2c1  \\\n",
       "0                     NaN                     NaN                     NaN   \n",
       "1                     NaN                     NaN                     NaN   \n",
       "2                     NaN                     NaN                     NaN   \n",
       "\n",
       "   size_tau2_0c1  size_tau2_1c1  size_tau2_3c1  size_tau2_4c1  size_tau2_2c1  \n",
       "0            NaN       0.301511            NaN            NaN            NaN  \n",
       "1            NaN            NaN            NaN            NaN            NaN  \n",
       "2            NaN            NaN            NaN            NaN            NaN  \n",
       "\n",
       "[3 rows x 247 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fe449383",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:28:14.528452Z",
     "iopub.status.busy": "2021-09-27T11:28:14.527588Z",
     "iopub.status.idle": "2021-09-27T11:28:14.531871Z",
     "shell.execute_reply": "2021-09-27T11:28:14.531477Z"
    },
    "papermill": {
     "duration": 0.060562,
     "end_time": "2021-09-27T11:28:14.531977",
     "exception": false,
     "start_time": "2021-09-27T11:28:14.471415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import glob\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "x_test = test.drop(['row_id', 'time_id'], axis = 1)\n",
    "x_test['stock_id'] = x_test['stock_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf0732a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:28:14.638752Z",
     "iopub.status.busy": "2021-09-27T11:28:14.638191Z",
     "iopub.status.idle": "2021-09-27T11:28:15.156886Z",
     "shell.execute_reply": "2021-09-27T11:28:15.155916Z"
    },
    "papermill": {
     "duration": 0.5736,
     "end_time": "2021-09-27T11:28:15.157037",
     "exception": false,
     "start_time": "2021-09-27T11:28:14.583437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_DIR =  \"../input/lk-wwdd\"\n",
    "lk_ffgg1 = np.zeros(len(x_test))\n",
    "files = glob.glob(f'{MODEL_DIR}/*model*.pkl')\n",
    "for i, f in enumerate(files):\n",
    "    model = joblib.load(f)\n",
    "    lk_ffgg1 += model.predict(x_test)\n",
    "lk_ffgg1=lk_ffgg1/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "78b8f5d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:28:15.268181Z",
     "iopub.status.busy": "2021-09-27T11:28:15.266956Z",
     "iopub.status.idle": "2021-09-27T11:28:15.270677Z",
     "shell.execute_reply": "2021-09-27T11:28:15.269902Z"
    },
    "papermill": {
     "duration": 0.062838,
     "end_time": "2021-09-27T11:28:15.270837",
     "exception": false,
     "start_time": "2021-09-27T11:28:15.207999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00114634 0.00251188 0.00251188]\n",
      "[0.00241323 0.00311746 0.00311746]\n",
      "[0.00094421 0.00098578 0.00098578]\n",
      "[0.00078814 0.00242929 0.00242929]\n",
      "[0.00195556 0.00308201 0.00308201]\n",
      "[0.0014088  0.00149299 0.00149299]\n"
     ]
    }
   ],
   "source": [
    "print(ng_ffgg1)\n",
    "print(tg_ffgg1)\n",
    "print(lg_ffgg1)\n",
    "print(nk_ffgg1)\n",
    "print(tk_ffgg1)\n",
    "print(lk_ffgg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9b24bc1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:28:15.380136Z",
     "iopub.status.busy": "2021-09-27T11:28:15.379652Z",
     "iopub.status.idle": "2021-09-27T11:28:15.385001Z",
     "shell.execute_reply": "2021-09-27T11:28:15.384607Z"
    },
    "papermill": {
     "duration": 0.062336,
     "end_time": "2021-09-27T11:28:15.385107",
     "exception": false,
     "start_time": "2021-09-27T11:28:15.322771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test['target'] = np.median([ng_ffgg1,tg_ffgg1,lg_ffgg1,nk_ffgg1,tk_ffgg1,lk_ffgg1],axis=0)\n",
    "test[['row_id', 'target']].to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0619bfbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-27T11:28:15.495866Z",
     "iopub.status.busy": "2021-09-27T11:28:15.495225Z",
     "iopub.status.idle": "2021-09-27T11:28:15.498238Z",
     "shell.execute_reply": "2021-09-27T11:28:15.498649Z"
    },
    "papermill": {
     "duration": 0.063035,
     "end_time": "2021-09-27T11:28:15.498767",
     "exception": false,
     "start_time": "2021-09-27T11:28:15.435732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-4</td>\n",
       "      <td>0.001278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-32</td>\n",
       "      <td>0.002471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-34</td>\n",
       "      <td>0.002471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  row_id    target\n",
       "0    0-4  0.001278\n",
       "1   0-32  0.002471\n",
       "2   0-34  0.002471"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[['row_id', 'target']].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 144.735835,
   "end_time": "2021-09-27T11:28:18.912056",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-27T11:25:54.176221",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
